{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60c71d3",
   "metadata": {},
   "source": [
    "# Import data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7f9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast # for conversin of a string representation of a list to a list\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') # Spacy language model\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx # for PageRank\n",
    "    \n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "#test_df.drop([8, 16, 28, 38, 39], axis = 0, inplace = True)\n",
    "#test_df.reset_index(inplace = True)\n",
    "#test_df = test_df[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ade33a",
   "metadata": {},
   "source": [
    "# Find good examples for modelling (sufficiently long abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9670771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ast.literal_eval(test_df.iloc[99][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d76aed",
   "metadata": {},
   "source": [
    "### Index -- Length of main text in sentences (> 50) -- length of abstract in sentences (> 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f70383",
   "metadata": {},
   "source": [
    "8%--12% of the abstract length from the main text is chosen for testing the models.\n",
    "\n",
    "0 -- 84 -- 7 (8%)\n",
    "\n",
    "1 -- 91 -- 5 (no -- too small abstract)\n",
    "\n",
    "2 -- 116 -- 10 (9%)\n",
    "\n",
    "3 -- 32 -- 9 (no -- too small main text)\n",
    "\n",
    "4 -- 91 -- 6 (no -- 7%)\n",
    "\n",
    "5 -- 102 -- 9 (9%)\n",
    "\n",
    "6 -- 63 -- 11 (no -- 17%)\n",
    "\n",
    "7 -- 105 -- 11 (10%)\n",
    "\n",
    "8 -- 123 -- 1 (no)\n",
    "\n",
    "9 -- 149 -- 6 (no -- 4%)\n",
    "\n",
    "10 -- 70 -- 14 (no)\n",
    "\n",
    "11 -- 92 -- 10 (11%)\n",
    "\n",
    "12 -- 222 -- 6 (no)\n",
    "\n",
    "13 -- 147 -- 5 (no)\n",
    "\n",
    "14 -- 150 -- 5 (no)\n",
    "\n",
    "15 -- 23 (no)\n",
    "\n",
    "16 -- 36 (no)\n",
    "\n",
    "17 -- 44 (no)\n",
    "\n",
    "18 -- 13 (no)\n",
    "\n",
    "19 -- 117 -- 9 (8%)\n",
    "\n",
    "20 -- 76 -- 4 (no)\n",
    "\n",
    "21 -- 53 -- 3 (no)\n",
    "\n",
    "22 -- 81 -- 10 (12%)\n",
    "\n",
    "23 -- 135 -- 11 (8%)\n",
    "\n",
    "24 -- 51 -- 7 (no -- 14%)\n",
    "\n",
    "25 -- 39 (no)\n",
    "\n",
    "26 -- 102 -- 1 (no)\n",
    "\n",
    "27 -- 70 -- 11 (no -- 16%)\n",
    "\n",
    "28 -- 41 (no)\n",
    "\n",
    "29 -- 131 -- 8 (no -- 6%)\n",
    "\n",
    "30 -- 43 (no)\n",
    "\n",
    "31 -- 209 -- 10 (no)\n",
    "\n",
    "32 -- 55 -- 16 (no)\n",
    "\n",
    "33 -- 63 -- 7 (11%)\n",
    "\n",
    "34 -- 42 (no)\n",
    "\n",
    "35 -- 40 (no)\n",
    "\n",
    "36 -- 132 -- 9 (no -- 7%)\n",
    "\n",
    "37 -- 75 -- 5 (no -- 7%)\n",
    "\n",
    "39 -- 56 -- 6 (11%)\n",
    "\n",
    "40 -- 59 -- 6 (19%)\n",
    "\n",
    "42 -- 92 -- 10 (11%)\n",
    "\n",
    "44 -- 100 -- 5(no -- 5%)\n",
    "\n",
    "46 -- 68 -- 12 (no)\n",
    "\n",
    "52 -- 143 -- 12 (8%)\n",
    "\n",
    "54 -- 53 -- 7 (no -- 13%)\n",
    "\n",
    "60 -- 100 -- 11 (11%)\n",
    "\n",
    "61 -- 92 -- 10 (11%)\n",
    "\n",
    "65 -- 78 -- 6 (8%)\n",
    "\n",
    "69 -- 121 -- 7 (no -- 6%)\n",
    "\n",
    "72 -- 105 -- 8 (8%)\n",
    "\n",
    "73 -- 58 -- 7 (12%)\n",
    "\n",
    "76 -- 127 -- 8 (no -- 6%)\n",
    "\n",
    "77 -- 99 -- 7 (no -- 7%)\n",
    "\n",
    "82 -- 95 -- 8 (8%)\n",
    "\n",
    "85 -- 148 -- 9 (no -- 6%)\n",
    "\n",
    "86 -- 76 -- 9 (12%)\n",
    "\n",
    "87 -- 109 -- 10 (9%)\n",
    "\n",
    "92 -- 51 -- 5 (10%)\n",
    "\n",
    "95 -- 52 -- 5 (10%)\n",
    "\n",
    "98 -- 92 -- 10 (11%)\n",
    "\n",
    "99 -- 102 -- 10 (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02c0e0",
   "metadata": {},
   "source": [
    "The chosen indices are 0, 2, 5, 7, 11, 19, 22, 23, 33, 39, 40, 42, 52, 60, 61, 65, 72, 73, 82, 86, 87, 92, 95, 98, 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97951b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09803921568627451"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10/102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a416bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.iloc[[0, 2, 5, 7, 11, 19, 22, 23, 33, 39, 40, 42, 52, 60, 61, 65, 72, 73, 82, 86, 87, 92, 95, 98, 99]]\n",
    "\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "test_df.drop(20, axis = 0, inplace = True)\n",
    "test_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f0b07",
   "metadata": {},
   "source": [
    "# Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a584eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    \"\"\"Return ROUGE, BLEU, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        result: dictionary with ROUGE, BLEU, and F1 score\n",
    "    \"\"\"\n",
    "    freq_ROUGE = 0\n",
    "    for each in gold:\n",
    "        if each in pred:\n",
    "            freq_ROUGE += 1\n",
    "    # portion of the words from golden summary appering in the generated summary\n",
    "    ROUGE = freq_ROUGE/len(gold) \n",
    "    #print(\"--------------ROUGE (Recall):\")\n",
    "    #print(f\"{round(ROUGE*100, 2)}%\")\n",
    "    \n",
    "    # Brevity penalised frequency\n",
    "    pred_count = {}\n",
    "    for each in set(pred):\n",
    "        pred_count[each] = min(gold.count(each), pred.count(each))\n",
    "\n",
    "    freq_BLEU = sum(pred_count.values())\n",
    "    \n",
    "    # (with brevity penalty) portion of the words from generated summary appering in the generated summary\n",
    "    BLEU = freq_BLEU/len(pred) \n",
    "    #print(\"--------------BLEU (Precision):\")\n",
    "    #print(f\"{round(BLEU*100, 2)}%\")\n",
    "\n",
    "    if ROUGE == 0 or BLEU == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*(ROUGE * BLEU)/(ROUGE + BLEU)\n",
    "    #print(\"--------------F1 score:\")\n",
    "    #print(f\"{round(f1*100, 2)}%\")\n",
    "    \n",
    "    return {'ROUGE' : round(ROUGE*100, 2), 'BLEU' : round(BLEU*100, 2), 'F1' : round(f1*100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56c209",
   "metadata": {},
   "source": [
    "# Text and summary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab92ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text = test_df['article_text'][0], summary = test_df['abstract_text'][0]):\n",
    "    \"\"\"Return text and summary before and after preprocessing \n",
    "    (stop word removal, non-alphabetic characters removal, and lemmatisation).\n",
    "    \n",
    "    Args:\n",
    "        text: String representaion of a list of sentences in the article body.\n",
    "        summary: String representaion of a list of sentences of a human-written abstract.\n",
    "    \n",
    "    Returns:\n",
    "        dictionary with text, summary, preprocessed text, and preprocessed summary.\n",
    "    \"\"\"\n",
    "    text = ast.literal_eval(text) # Convert sttring representatino of a list to a list\n",
    "    summary = ast.literal_eval(summary)\n",
    "\n",
    "    # Remove html tags\n",
    "    \n",
    "    summary = [sent.replace(\" </S>\", \"\") for sent in summary] \n",
    "    summary = [sent.replace(\"<S> \", \"\") for sent in summary]\n",
    "\n",
    "    # Text preprocessing\n",
    "    \n",
    "    text_prep = []\n",
    "    for sent in text:\n",
    "        doc = nlp(sent)\n",
    "        doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "        doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "        text_prep.append(\" \".join(doc))\n",
    "\n",
    "    # Abstract preprocessing\n",
    "\n",
    "    summary_prep = []\n",
    "    for sent in summary:\n",
    "        doc = nlp(sent)\n",
    "        doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "        doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "        summary_prep.append(\" \".join(doc))\n",
    "        \n",
    "    return {'text' : text, 'summary' : summary, 'text_prep' : text_prep, 'summary_prep' : summary_prep}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048dead",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0545062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(sentences):\n",
    "    \"\"\"TextRank algorithm. Rank sentences using PageRank \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = {}\n",
    "\n",
    "    for ind in range(len(sentences)): \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec[ind] = bow\n",
    "        \n",
    "    sim_mat = np.zeros((len(sentences), len(sentences)))\n",
    "    for ind_1 in range(len(sentences)):\n",
    "        for ind_2 in range(ind_1, len(sentences)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f0090",
   "metadata": {},
   "source": [
    "# WordRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da74b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordrank(sentences):\n",
    "    \"\"\"WordRank algorithm. Rank sentences using PageRank \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words)) # Find unique words in all processed sentences\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    # Find sentence scores as sum of all included word scores\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48119472",
   "metadata": {},
   "source": [
    "# Hybrid64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8b001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid64(sentences):\n",
    "    \"\"\"Hybrid WordRank+TextRank algorithm. Rank sentences using PageRank. \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Retrieve important sentences indices\n",
    "\n",
    "    new_sent_ind = []\n",
    "    count = 0\n",
    "\n",
    "    for k in sorted_scores.keys():\n",
    "        if count > len(sorted_scores)*0.64:\n",
    "            break\n",
    "        else:\n",
    "            new_sent_ind.append(k)\n",
    "            count += 1\n",
    "            \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = []\n",
    "\n",
    "    for ind in new_sent_ind: \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec.append(bow) # reindexed. old index can be found via new_sent_ind[this index]\n",
    "        \n",
    "    sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "    for ind_1 in range(len(new_sent_ind)):\n",
    "        for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "    \n",
    "    # Need to return to old indices. Redefine keys of this dictionary:\n",
    "    new_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        new_scores[new_sent_ind[k]] = v\n",
    "\n",
    "    sorted_scores = dict(sorted(new_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627ad66",
   "metadata": {},
   "source": [
    "# Hybrid80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5936d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid80(sentences):\n",
    "    \"\"\"Hybrid WordRank+TextRank algorithm. Rank sentences using PageRank. \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Retrieve important sentences indices\n",
    "\n",
    "    new_sent_ind = []\n",
    "    count = 0\n",
    "\n",
    "    for k in sorted_scores.keys():\n",
    "        if count > len(sorted_scores)*0.8:\n",
    "            break\n",
    "        else:\n",
    "            new_sent_ind.append(k)\n",
    "            count += 1\n",
    "            \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = []\n",
    "\n",
    "    for ind in new_sent_ind: \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec.append(bow) # reindexed. old index can be found via new_sent_ind[this index]\n",
    "        \n",
    "    sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "    for ind_1 in range(len(new_sent_ind)):\n",
    "        for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "    \n",
    "    # Need to return to old indices. Redefine keys of this dictionary:\n",
    "    new_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        new_scores[new_sent_ind[k]] = v\n",
    "\n",
    "    sorted_scores = dict(sorted(new_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57145ac5",
   "metadata": {},
   "source": [
    "# Generate summary based on the ranked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f08e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sorted_scores, text, summary, text_prep):\n",
    "    \"\"\"Generate summary based on the top ranked sentences\n",
    "    \n",
    "    Args:\n",
    "        text_prep: List of preprocessed sentences to rank\n",
    "        text: List of initial sentences to rank\n",
    "        summary: golden summary\n",
    "        sorted_scores: sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \n",
    "    Returns:\n",
    "        Generated summaries: with and wirhout preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    top_sent = []\n",
    "    top_sent_prep = []\n",
    "    for k, v in sorted_scores.items():\n",
    "        # Number of sentences in generated summary equal to golden summary length\n",
    "        if len(top_sent_prep) < len(summary): \n",
    "            top_sent.append(text[k])\n",
    "            top_sent_prep.append(text_prep[k])\n",
    "        else:\n",
    "            break\n",
    "    return {'gen_summary' : top_sent, 'gen_summary_prep' : top_sent_prep}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8217b",
   "metadata": {},
   "source": [
    "# Obtain unigrams, bigrams, and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eca70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text):\n",
    "    \"\"\"Obtain unigrams, bigrams, and trigrams in the given text\n",
    "    \n",
    "    Args:\n",
    "        text: List of sentences\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with unigrams, bigrams, and trigrams\n",
    "    \"\"\"\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    #Unigrams\n",
    "    text_1 = text.split()\n",
    "\n",
    "    # Bigrams\n",
    "    text_2 = []\n",
    "\n",
    "    for i in range(len(text_1) - 1):\n",
    "        text_2.append(text_1[i] + \" \" + text_1[i + 1])\n",
    "\n",
    "    # Trigrams\n",
    "    text_3 = []\n",
    "\n",
    "    for i in range(len(text_1) - 2):\n",
    "        text_3.append(text_1[i] + \" \" + text_1[i + 1] +  \" \" + text_1[i + 2])\n",
    "    \n",
    "    return {'unigrams' : text_1, 'bigrams' : text_2, 'trigrams' : text_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65145b64",
   "metadata": {},
   "source": [
    "# TextRank algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424a5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 62.02088212966919 seconds ---\n"
     ]
    }
   ],
   "source": [
    "textrank_ROUGE_1 = []\n",
    "textrank_ROUGE_2 = []\n",
    "textrank_ROUGE_3 = []\n",
    "\n",
    "textrank_BLEU_1 = []\n",
    "textrank_BLEU_2 = []\n",
    "textrank_BLEU_3 = []\n",
    "\n",
    "textrank_F1_1 = []\n",
    "textrank_F1_2 = []\n",
    "textrank_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]):  #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = textrank(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    textrank_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    textrank_BLEU_1.append(metrics_1['BLEU'])\n",
    "    textrank_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    textrank_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    textrank_BLEU_2.append(metrics_2['BLEU'])\n",
    "    textrank_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    textrank_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    textrank_BLEU_3.append(metrics_3['BLEU'])\n",
    "    textrank_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "textrank_df = pd.DataFrame({'ROUGE_1' : textrank_ROUGE_1, 'ROUGE_2' : textrank_ROUGE_2, 'ROUGE_3' : textrank_ROUGE_3, \n",
    "                            'BLEU_1' : textrank_BLEU_1, 'BLEU_2' : textrank_BLEU_2, 'BLEU_3' : textrank_BLEU_3, \n",
    "                            'F1_1' : textrank_F1_1, 'F1_2' : textrank_F1_2, 'F1_3': textrank_F1_3})\n",
    "\n",
    "textrank_df.to_csv(\"../data/textrank_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6734c6",
   "metadata": {},
   "source": [
    "# WordRank algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be55722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 935.7120773792267 seconds ---\n"
     ]
    }
   ],
   "source": [
    "wordrank_ROUGE_1 = []\n",
    "wordrank_ROUGE_2 = []\n",
    "wordrank_ROUGE_3 = []\n",
    "\n",
    "wordrank_BLEU_1 = []\n",
    "wordrank_BLEU_2 = []\n",
    "wordrank_BLEU_3 = []\n",
    "\n",
    "wordrank_F1_1 = []\n",
    "wordrank_F1_2 = []\n",
    "wordrank_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = wordrank(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    wordrank_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    wordrank_BLEU_1.append(metrics_1['BLEU'])\n",
    "    wordrank_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    wordrank_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    wordrank_BLEU_2.append(metrics_2['BLEU'])\n",
    "    wordrank_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    wordrank_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    wordrank_BLEU_3.append(metrics_3['BLEU'])\n",
    "    wordrank_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "wordrank_df = pd.DataFrame({'ROUGE_1' : wordrank_ROUGE_1, 'ROUGE_2' : wordrank_ROUGE_2, 'ROUGE_3' : wordrank_ROUGE_3, \n",
    "                            'BLEU_1' : wordrank_BLEU_1, 'BLEU_2' : wordrank_BLEU_2, 'BLEU_3' : wordrank_BLEU_3, \n",
    "                            'F1_1' : wordrank_F1_1, 'F1_2' : wordrank_F1_2, 'F1_3': wordrank_F1_3})\n",
    "\n",
    "wordrank_df.to_csv(\"../data/wordrank_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eef040",
   "metadata": {},
   "source": [
    "# Hybrid64 algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c389bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 946.6000809669495 seconds ---\n"
     ]
    }
   ],
   "source": [
    "hybrid64_ROUGE_1 = []\n",
    "hybrid64_ROUGE_2 = []\n",
    "hybrid64_ROUGE_3 = []\n",
    "\n",
    "hybrid64_BLEU_1 = []\n",
    "hybrid64_BLEU_2 = []\n",
    "hybrid64_BLEU_3 = []\n",
    "\n",
    "hybrid64_F1_1 = []\n",
    "hybrid64_F1_2 = []\n",
    "hybrid64_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = hybrid64(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    hybrid64_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    hybrid64_BLEU_1.append(metrics_1['BLEU'])\n",
    "    hybrid64_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    hybrid64_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    hybrid64_BLEU_2.append(metrics_2['BLEU'])\n",
    "    hybrid64_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    hybrid64_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    hybrid64_BLEU_3.append(metrics_3['BLEU'])\n",
    "    hybrid64_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "hybrid64_df = pd.DataFrame({'ROUGE_1' : hybrid64_ROUGE_1, 'ROUGE_2' : hybrid64_ROUGE_2, 'ROUGE_3' : hybrid64_ROUGE_3, \n",
    "                            'BLEU_1' : hybrid64_BLEU_1, 'BLEU_2' : hybrid64_BLEU_2, 'BLEU_3' : hybrid64_BLEU_3, \n",
    "                            'F1_1' : hybrid64_F1_1, 'F1_2' : hybrid64_F1_2, 'F1_3': hybrid64_F1_3})\n",
    "\n",
    "hybrid64_df.to_csv(\"../data/hybrid64_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7995fb",
   "metadata": {},
   "source": [
    "# Hybrid80 algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48c232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 959.6928148269653 seconds ---\n"
     ]
    }
   ],
   "source": [
    "hybrid80_ROUGE_1 = []\n",
    "hybrid80_ROUGE_2 = []\n",
    "hybrid80_ROUGE_3 = []\n",
    "\n",
    "hybrid80_BLEU_1 = []\n",
    "hybrid80_BLEU_2 = []\n",
    "hybrid80_BLEU_3 = []\n",
    "\n",
    "hybrid80_F1_1 = []\n",
    "hybrid80_F1_2 = []\n",
    "hybrid80_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = hybrid80(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    hybrid80_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    hybrid80_BLEU_1.append(metrics_1['BLEU'])\n",
    "    hybrid80_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    hybrid80_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    hybrid80_BLEU_2.append(metrics_2['BLEU'])\n",
    "    hybrid80_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    hybrid80_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    hybrid80_BLEU_3.append(metrics_3['BLEU'])\n",
    "    hybrid80_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "hybrid80_df = pd.DataFrame({'ROUGE_1' : hybrid80_ROUGE_1, 'ROUGE_2' : hybrid80_ROUGE_2, 'ROUGE_3' : hybrid80_ROUGE_3, \n",
    "                            'BLEU_1' : hybrid80_BLEU_1, 'BLEU_2' : hybrid80_BLEU_2, 'BLEU_3' : hybrid80_BLEU_3, \n",
    "                            'F1_1' : hybrid80_F1_1, 'F1_2' : hybrid80_F1_2, 'F1_3': hybrid80_F1_3})\n",
    "\n",
    "hybrid80_df.to_csv(\"../data/hybrid80_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
