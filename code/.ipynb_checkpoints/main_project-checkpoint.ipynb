{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60c71d3",
   "metadata": {},
   "source": [
    "# Import data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc7f9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast # for conversin of a string representation of a list to a list\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') # Spacy language model\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx # for PageRank\n",
    "    \n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "#test_df.drop([8, 16, 28, 38, 39], axis = 0, inplace = True)\n",
    "#test_df.reset_index(inplace = True)\n",
    "#test_df = test_df[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ddc42",
   "metadata": {},
   "source": [
    "# Find good examples for modelling (sufficiently long abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f75e0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ast.literal_eval(test_df.iloc[99][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b8ec0",
   "metadata": {},
   "source": [
    "### Index -- Length of main text in sentences (> 50) -- length of abstract in sentences (> 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75668b7d",
   "metadata": {},
   "source": [
    "8%--12% of the abstract length from the main text is chosen for testing the models.\n",
    "\n",
    "0 -- 84 -- 7 (8%)\n",
    "\n",
    "1 -- 91 -- 5 (no -- too small abstract)\n",
    "\n",
    "2 -- 116 -- 10 (9%)\n",
    "\n",
    "3 -- 32 -- 9 (no -- too small main text)\n",
    "\n",
    "4 -- 91 -- 6 (no -- 7%)\n",
    "\n",
    "5 -- 102 -- 9 (9%)\n",
    "\n",
    "6 -- 63 -- 11 (no -- 17%)\n",
    "\n",
    "7 -- 105 -- 11 (10%)\n",
    "\n",
    "8 -- 123 -- 1 (no)\n",
    "\n",
    "9 -- 149 -- 6 (no -- 4%)\n",
    "\n",
    "10 -- 70 -- 14 (no)\n",
    "\n",
    "11 -- 92 -- 10 (11%)\n",
    "\n",
    "12 -- 222 -- 6 (no)\n",
    "\n",
    "13 -- 147 -- 5 (no)\n",
    "\n",
    "14 -- 150 -- 5 (no)\n",
    "\n",
    "15 -- 23 (no)\n",
    "\n",
    "16 -- 36 (no)\n",
    "\n",
    "17 -- 44 (no)\n",
    "\n",
    "18 -- 13 (no)\n",
    "\n",
    "19 -- 117 -- 9 (8%)\n",
    "\n",
    "20 -- 76 -- 4 (no)\n",
    "\n",
    "21 -- 53 -- 3 (no)\n",
    "\n",
    "22 -- 81 -- 10 (12%)\n",
    "\n",
    "23 -- 135 -- 11 (8%)\n",
    "\n",
    "24 -- 51 -- 7 (no -- 14%)\n",
    "\n",
    "25 -- 39 (no)\n",
    "\n",
    "26 -- 102 -- 1 (no)\n",
    "\n",
    "27 -- 70 -- 11 (no -- 16%)\n",
    "\n",
    "28 -- 41 (no)\n",
    "\n",
    "29 -- 131 -- 8 (no -- 6%)\n",
    "\n",
    "30 -- 43 (no)\n",
    "\n",
    "31 -- 209 -- 10 (no)\n",
    "\n",
    "32 -- 55 -- 16 (no)\n",
    "\n",
    "33 -- 63 -- 7 (11%)\n",
    "\n",
    "34 -- 42 (no)\n",
    "\n",
    "35 -- 40 (no)\n",
    "\n",
    "36 -- 132 -- 9 (no -- 7%)\n",
    "\n",
    "37 -- 75 -- 5 (no -- 7%)\n",
    "\n",
    "39 -- 56 -- 6 (11%)\n",
    "\n",
    "40 -- 59 -- 6 (19%)\n",
    "\n",
    "42 -- 92 -- 10 (11%)\n",
    "\n",
    "44 -- 100 -- 5(no -- 5%)\n",
    "\n",
    "46 -- 68 -- 12 (no)\n",
    "\n",
    "52 -- 143 -- 12 (8%)\n",
    "\n",
    "54 -- 53 -- 7 (no -- 13%)\n",
    "\n",
    "60 -- 100 -- 11 (11%)\n",
    "\n",
    "61 -- 92 -- 10 (11%)\n",
    "\n",
    "65 -- 78 -- 6 (8%)\n",
    "\n",
    "69 -- 121 -- 7 (no -- 6%)\n",
    "\n",
    "72 -- 105 -- 8 (8%)\n",
    "\n",
    "73 -- 58 -- 7 (12%)\n",
    "\n",
    "76 -- 127 -- 8 (no -- 6%)\n",
    "\n",
    "77 -- 99 -- 7 (no -- 7%)\n",
    "\n",
    "82 -- 95 -- 8 (8%)\n",
    "\n",
    "85 -- 148 -- 9 (no -- 6%)\n",
    "\n",
    "86 -- 76 -- 9 (12%)\n",
    "\n",
    "87 -- 109 -- 10 (9%)\n",
    "\n",
    "92 -- 51 -- 5 (10%)\n",
    "\n",
    "95 -- 52 -- 5 (10%)\n",
    "\n",
    "98 -- 92 -- 10 (11%)\n",
    "\n",
    "99 -- 102 -- 10 (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043bc76",
   "metadata": {},
   "source": [
    "The chosen indices are 0, 2, 5, 7, 11, 19, 22, 23, 33, 39, 40, 42, 52, 60, 61, 65, 72, 73, 82, 86, 87, 92, 95, 98, 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896f7c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09803921568627451"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10/102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9a5a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.iloc[[0, 2, 5, 7, 11, 19, 22, 23, 33, 39, 40, 42, 52, 60, 61, 65, 72, 73, 82, 86, 87, 92, 95, 98, 99]]\n",
    "\n",
    "test_df.reset_index(inplace = True, drop = True)\n",
    "test_df.drop(20, axis = 0, inplace = True)\n",
    "test_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "85b734fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"<S> research on the implications of anxiety in parkinson \\'s disease ( pd ) has been neglected despite its prevalence in nearly 50% of patients and its negative impact on quality of life . </S>\", \\'<S> previous reports have noted that neuropsychiatric symptoms impair cognitive performance in pd patients ; however , to date , no study has directly compared pd patients with and without anxiety to examine the impact of anxiety on cognitive impairments in pd . </S>\\', \\'<S> this study compared cognitive performance across 50 pd participants with and without anxiety ( 17 pda+ ; 33 pda ) , who underwent neurological and neuropsychological assessment . </S>\\', \\'<S> group performance was compared across the following cognitive domains : simple attention / visuomotor processing speed , executive function ( e.g. , set - shifting ) , working memory , language , and memory / new verbal learning . </S>\\', \\'<S> results showed that pda+ performed significantly worse on the digit span forward and backward test and part b of the trail making task ( tmt - b ) compared to the pda group . </S>\\', \\'<S> there were no group differences in verbal fluency , logical memory , or tmt - a performance . in conclusion , </S>\\', \\'<S> anxiety in pd has a measurable impact on working memory and attentional set - shifting . </S>\\']'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f0b07",
   "metadata": {},
   "source": [
    "# Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a584eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    \"\"\"Return ROUGE, BLEU, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        result: dictionary with ROUGE, BLEU, and F1 score\n",
    "    \"\"\"\n",
    "    freq_ROUGE = 0\n",
    "    for each in gold:\n",
    "        if each in pred:\n",
    "            freq_ROUGE += 1\n",
    "    # portion of the words from golden summary appering in the generated summary\n",
    "    ROUGE = freq_ROUGE/len(gold) \n",
    "    #print(\"--------------ROUGE (Recall):\")\n",
    "    #print(f\"{round(ROUGE*100, 2)}%\")\n",
    "    \n",
    "    # Brevity penalised frequency\n",
    "    pred_count = {}\n",
    "    for each in set(pred):\n",
    "        pred_count[each] = min(gold.count(each), pred.count(each))\n",
    "\n",
    "    freq_BLEU = sum(pred_count.values())\n",
    "    \n",
    "    # (with brevity penalty) portion of the words from generated summary appering in the generated summary\n",
    "    BLEU = freq_BLEU/len(pred) \n",
    "    #print(\"--------------BLEU (Precision):\")\n",
    "    #print(f\"{round(BLEU*100, 2)}%\")\n",
    "\n",
    "    if ROUGE == 0 or BLEU == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*(ROUGE * BLEU)/(ROUGE + BLEU)\n",
    "    #print(\"--------------F1 score:\")\n",
    "    #print(f\"{round(f1*100, 2)}%\")\n",
    "    \n",
    "    return {'ROUGE' : round(ROUGE*100, 2), 'BLEU' : round(BLEU*100, 2), 'F1' : round(f1*100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56c209",
   "metadata": {},
   "source": [
    "# Text and summary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ab92ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text = test_df['article_text'][0], summary = test_df['abstract_text'][0]):\n",
    "    \"\"\"Return text and summary before and after preprocessing \n",
    "    (stop word removal, non-alphabetic characters removal, and lemmatisation).\n",
    "    \n",
    "    Args:\n",
    "        text: String representaion of a list of sentences in the article body.\n",
    "        summary: String representaion of a list of sentences of a human-written abstract.\n",
    "    \n",
    "    Returns:\n",
    "        dictionary with text, summary, preprocessed text, and preprocessed summary.\n",
    "    \"\"\"\n",
    "    text = ast.literal_eval(text) # Convert sttring representatino of a list to a list\n",
    "    summary = ast.literal_eval(summary)\n",
    "\n",
    "    # Remove html tags\n",
    "    \n",
    "    summary = [sent.replace(\" </S>\", \"\") for sent in summary] \n",
    "    summary = [sent.replace(\"<S> \", \"\") for sent in summary]\n",
    "\n",
    "    # Text preprocessing\n",
    "    \n",
    "    text_prep = []\n",
    "    for sent in text:\n",
    "        doc = nlp(sent)\n",
    "        doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "        doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "        text_prep.append(\" \".join(doc))\n",
    "\n",
    "    # Abstract preprocessing\n",
    "\n",
    "    summary_prep = []\n",
    "    for sent in summary:\n",
    "        doc = nlp(sent)\n",
    "        doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "        doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "        summary_prep.append(\" \".join(doc))\n",
    "        \n",
    "    return {'text' : text, 'summary' : summary, 'text_prep' : text_prep, 'summary_prep' : summary_prep}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048dead",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b0545062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(sentences):\n",
    "    \"\"\"TextRank algorithm. Rank sentences using PageRank \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = {}\n",
    "\n",
    "    for ind in range(len(sentences)): \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec[ind] = bow\n",
    "        \n",
    "    sim_mat = np.zeros((len(sentences), len(sentences)))\n",
    "    for ind_1 in range(len(sentences)):\n",
    "        for ind_2 in range(ind_1, len(sentences)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f0090",
   "metadata": {},
   "source": [
    "# WordRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1da74b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordrank(sentences):\n",
    "    \"\"\"WordRank algorithm. Rank sentences using PageRank \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words)) # Find unique words in all processed sentences\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    # Find sentence scores as sum of all included word scores\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48119472",
   "metadata": {},
   "source": [
    "# Hybrid64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cc8b001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid64(sentences):\n",
    "    \"\"\"Hybrid WordRank+TextRank algorithm. Rank sentences using PageRank. \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Retrieve important sentences indices\n",
    "\n",
    "    new_sent_ind = []\n",
    "    count = 0\n",
    "\n",
    "    for k in sorted_scores.keys():\n",
    "        if count > len(sorted_scores)*0.64:\n",
    "            break\n",
    "        else:\n",
    "            new_sent_ind.append(k)\n",
    "            count += 1\n",
    "            \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = []\n",
    "\n",
    "    for ind in new_sent_ind: \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec.append(bow) # reindexed. old index can be found via new_sent_ind[this index]\n",
    "        \n",
    "    sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "    for ind_1 in range(len(new_sent_ind)):\n",
    "        for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "    \n",
    "    # Need to return to old indices. Redefine keys of this dictionary:\n",
    "    new_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        new_scores[new_sent_ind[k]] = v\n",
    "\n",
    "    sorted_scores = dict(sorted(new_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627ad66",
   "metadata": {},
   "source": [
    "# Hybrid80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5936d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid80(sentences):\n",
    "    \"\"\"Hybrid WordRank+TextRank algorithm. Rank sentences using PageRank. \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Retrieve important sentences indices\n",
    "\n",
    "    new_sent_ind = []\n",
    "    count = 0\n",
    "\n",
    "    for k in sorted_scores.keys():\n",
    "        if count > len(sorted_scores)*0.8:\n",
    "            break\n",
    "        else:\n",
    "            new_sent_ind.append(k)\n",
    "            count += 1\n",
    "            \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = []\n",
    "\n",
    "    for ind in new_sent_ind: \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec.append(bow) # reindexed. old index can be found via new_sent_ind[this index]\n",
    "        \n",
    "    sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "    for ind_1 in range(len(new_sent_ind)):\n",
    "        for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "    \n",
    "    # Need to return to old indices. Redefine keys of this dictionary:\n",
    "    new_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        new_scores[new_sent_ind[k]] = v\n",
    "\n",
    "    sorted_scores = dict(sorted(new_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57145ac5",
   "metadata": {},
   "source": [
    "# Generate summary based on the ranked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "54f08e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sorted_scores, text, summary, text_prep):\n",
    "    \"\"\"Generate summary based on the top ranked sentences\n",
    "    \n",
    "    Args:\n",
    "        text_prep: List of preprocessed sentences to rank\n",
    "        text: List of initial sentences to rank\n",
    "        summary: golden summary\n",
    "        sorted_scores: sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \n",
    "    Returns:\n",
    "        Generated summaries: with and wirhout preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    top_sent = []\n",
    "    top_sent_prep = []\n",
    "    for k, v in sorted_scores.items():\n",
    "        # Number of sentences in generated summary equal to golden summary length\n",
    "        if len(top_sent_prep) < len(summary): \n",
    "            top_sent.append(text[k])\n",
    "            top_sent_prep.append(text_prep[k])\n",
    "        else:\n",
    "            break\n",
    "    return {'gen_summary' : top_sent, 'gen_summary_prep' : top_sent_prep}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8217b",
   "metadata": {},
   "source": [
    "# Obtain unigrams, bigrams, and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4eca70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text):\n",
    "    \"\"\"Obtain unigrams, bigrams, and trigrams in the given text\n",
    "    \n",
    "    Args:\n",
    "        text: List of sentences\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with unigrams, bigrams, and trigrams\n",
    "    \"\"\"\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    #Unigrams\n",
    "    text_1 = text.split()\n",
    "\n",
    "    # Bigrams\n",
    "    text_2 = []\n",
    "\n",
    "    for i in range(len(text_1) - 1):\n",
    "        text_2.append(text_1[i] + \" \" + text_1[i + 1])\n",
    "\n",
    "    # Trigrams\n",
    "    text_3 = []\n",
    "\n",
    "    for i in range(len(text_1) - 2):\n",
    "        text_3.append(text_1[i] + \" \" + text_1[i + 1] +  \" \" + text_1[i + 2])\n",
    "    \n",
    "    return {'unigrams' : text_1, 'bigrams' : text_2, 'trigrams' : text_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65145b64",
   "metadata": {},
   "source": [
    "# TextRank algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424a5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 62.02088212966919 seconds ---\n"
     ]
    }
   ],
   "source": [
    "textrank_ROUGE_1 = []\n",
    "textrank_ROUGE_2 = []\n",
    "textrank_ROUGE_3 = []\n",
    "\n",
    "textrank_BLEU_1 = []\n",
    "textrank_BLEU_2 = []\n",
    "textrank_BLEU_3 = []\n",
    "\n",
    "textrank_F1_1 = []\n",
    "textrank_F1_2 = []\n",
    "textrank_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]):  #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = textrank(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    textrank_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    textrank_BLEU_1.append(metrics_1['BLEU'])\n",
    "    textrank_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    textrank_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    textrank_BLEU_2.append(metrics_2['BLEU'])\n",
    "    textrank_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    textrank_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    textrank_BLEU_3.append(metrics_3['BLEU'])\n",
    "    textrank_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "textrank_df = pd.DataFrame({'ROUGE_1' : textrank_ROUGE_1, 'ROUGE_2' : textrank_ROUGE_2, 'ROUGE_3' : textrank_ROUGE_3, \n",
    "                            'BLEU_1' : textrank_BLEU_1, 'BLEU_2' : textrank_BLEU_2, 'BLEU_3' : textrank_BLEU_3, \n",
    "                            'F1_1' : textrank_F1_1, 'F1_2' : textrank_F1_2, 'F1_3': textrank_F1_3})\n",
    "\n",
    "textrank_df.to_csv(\"../data/textrank_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6948549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0333333333333334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "62/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6734c6",
   "metadata": {},
   "source": [
    "# WordRank algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be55722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 935.7120773792267 seconds ---\n"
     ]
    }
   ],
   "source": [
    "wordrank_ROUGE_1 = []\n",
    "wordrank_ROUGE_2 = []\n",
    "wordrank_ROUGE_3 = []\n",
    "\n",
    "wordrank_BLEU_1 = []\n",
    "wordrank_BLEU_2 = []\n",
    "wordrank_BLEU_3 = []\n",
    "\n",
    "wordrank_F1_1 = []\n",
    "wordrank_F1_2 = []\n",
    "wordrank_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = wordrank(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    wordrank_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    wordrank_BLEU_1.append(metrics_1['BLEU'])\n",
    "    wordrank_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    wordrank_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    wordrank_BLEU_2.append(metrics_2['BLEU'])\n",
    "    wordrank_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    wordrank_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    wordrank_BLEU_3.append(metrics_3['BLEU'])\n",
    "    wordrank_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "wordrank_df = pd.DataFrame({'ROUGE_1' : wordrank_ROUGE_1, 'ROUGE_2' : wordrank_ROUGE_2, 'ROUGE_3' : wordrank_ROUGE_3, \n",
    "                            'BLEU_1' : wordrank_BLEU_1, 'BLEU_2' : wordrank_BLEU_2, 'BLEU_3' : wordrank_BLEU_3, \n",
    "                            'F1_1' : wordrank_F1_1, 'F1_2' : wordrank_F1_2, 'F1_3': wordrank_F1_3})\n",
    "\n",
    "wordrank_df.to_csv(\"../data/wordrank_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3180ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.595"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "935.7/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eef040",
   "metadata": {},
   "source": [
    "# Hybrid64 algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c389bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 946.6000809669495 seconds ---\n"
     ]
    }
   ],
   "source": [
    "hybrid64_ROUGE_1 = []\n",
    "hybrid64_ROUGE_2 = []\n",
    "hybrid64_ROUGE_3 = []\n",
    "\n",
    "hybrid64_BLEU_1 = []\n",
    "hybrid64_BLEU_2 = []\n",
    "hybrid64_BLEU_3 = []\n",
    "\n",
    "hybrid64_F1_1 = []\n",
    "hybrid64_F1_2 = []\n",
    "hybrid64_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = hybrid64(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    hybrid64_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    hybrid64_BLEU_1.append(metrics_1['BLEU'])\n",
    "    hybrid64_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    hybrid64_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    hybrid64_BLEU_2.append(metrics_2['BLEU'])\n",
    "    hybrid64_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    hybrid64_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    hybrid64_BLEU_3.append(metrics_3['BLEU'])\n",
    "    hybrid64_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "hybrid64_df = pd.DataFrame({'ROUGE_1' : hybrid64_ROUGE_1, 'ROUGE_2' : hybrid64_ROUGE_2, 'ROUGE_3' : hybrid64_ROUGE_3, \n",
    "                            'BLEU_1' : hybrid64_BLEU_1, 'BLEU_2' : hybrid64_BLEU_2, 'BLEU_3' : hybrid64_BLEU_3, \n",
    "                            'F1_1' : hybrid64_F1_1, 'F1_2' : hybrid64_F1_2, 'F1_3': hybrid64_F1_3})\n",
    "\n",
    "hybrid64_df.to_csv(\"../data/hybrid64_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f0e16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.776666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "946.6/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7995fb",
   "metadata": {},
   "source": [
    "# Hybrid80 algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48c232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 959.6928148269653 seconds ---\n"
     ]
    }
   ],
   "source": [
    "hybrid80_ROUGE_1 = []\n",
    "hybrid80_ROUGE_2 = []\n",
    "hybrid80_ROUGE_3 = []\n",
    "\n",
    "hybrid80_BLEU_1 = []\n",
    "hybrid80_BLEU_2 = []\n",
    "hybrid80_BLEU_3 = []\n",
    "\n",
    "hybrid80_F1_1 = []\n",
    "hybrid80_F1_2 = []\n",
    "hybrid80_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = hybrid80(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    hybrid80_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    hybrid80_BLEU_1.append(metrics_1['BLEU'])\n",
    "    hybrid80_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    hybrid80_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    hybrid80_BLEU_2.append(metrics_2['BLEU'])\n",
    "    hybrid80_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    hybrid80_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    hybrid80_BLEU_3.append(metrics_3['BLEU'])\n",
    "    hybrid80_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "hybrid80_df = pd.DataFrame({'ROUGE_1' : hybrid80_ROUGE_1, 'ROUGE_2' : hybrid80_ROUGE_2, 'ROUGE_3' : hybrid80_ROUGE_3, \n",
    "                            'BLEU_1' : hybrid80_BLEU_1, 'BLEU_2' : hybrid80_BLEU_2, 'BLEU_3' : hybrid80_BLEU_3, \n",
    "                            'F1_1' : hybrid80_F1_1, 'F1_2' : hybrid80_F1_2, 'F1_3': hybrid80_F1_3})\n",
    "\n",
    "hybrid80_df.to_csv(\"../data/hybrid80_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925ee2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.995000000000001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "959.7/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c8fd4",
   "metadata": {},
   "source": [
    "# Bert Extractive Summarizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6490e",
   "metadata": {},
   "source": [
    "#### Colab Google was used with the following code to generate summaries:\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()\n",
    "\n",
    "text = \"article text preprocessed\"\n",
    "\n",
    "result = model(text, num_sentences=len(abstract_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d91f5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertsum = []\n",
    "\n",
    "bertsum.append(['anxiety affect quality life live parkinson disease pd overall cognitive status motor deficit apathy depression',\n",
    " 'furthermore trait anxiety negatively associate cognitive domain assess rban immediate memory visuospatial construction language attention delay memory',\n",
    " 'strike date study examine influence anxiety cognition pd directly compare group pd patient anxiety exclude depression',\n",
    " 'give research healthy young adult suggest anxiety reduce processing capacity impair processing efficiency especially central executive attentional system work memory hypothesize pd patient anxiety impairment attentional set shift working memory compare pd patient anxiety',\n",
    " 'self report had assess anxiety pd previously show useful measure clinical anxiety pd',\n",
    " 'spearman correlation perform separately group examine association anxiety depression rating cognitive function',\n",
    " 'recent functional neuroimaging work suggest enhance hippocampal activation executive functioning work memory task represent compensatory process impaired frontostriatal function pd patient compare control chronic stress anxiety',\n",
    " 'future study examine treat neuropsychiatric symptom impact progression cognitive decline improve cognitive impairment pd patient'])\n",
    "\n",
    "\n",
    "bertsum.append(['ohss complication ovulation induction occur vitro fertilization patient rizk aboulghar brinsden et al',\n",
    " 'iatrogenic condition spectrum clinical laboratory manifestation range mild severe life threaten condition rizk et al manifestation ohss ascite pleural effusion rizk smitz',\n",
    " 'ascite result increase abdominal pressure compromise venous return cardiac output renal perfusion navot et al',\n",
    " 'care take patient fluid drain peritoneal cavity',\n",
    " 'demographic datum include mean age year duration infertility year bmi kg aetiology infertility illustrate table ovarian stimulation datum include number treatment day total dose recombinant follicle stimulate hormone hormone serum level endometrial thickness number follicle number patient receive iu human chorionic gonadotrophin illustrate table ii',\n",
    " 'moderate ohss usually require intervention outpatient monitoring essential identify progression severe form rizk',\n",
    " 'patient present severe form ohss usually manage inpatient basis patient critical ohss admit intensive care unit aggressive supportive measure avoid maternal morbidity mortality practice committee american society reproductive medicine',\n",
    " 'hespan caution patient late onset ohss benefit outweigh fact outpatient management early severe ohss common practice infertility specialist trans vaginal aspiration ascitic fluid iv fluid hydration',\n",
    " 'recently report maternal mortality rate ohss netherlands united kingdom demonstrate incidence approximately death ivf cycle braat et al',\n",
    " 'addition lack control group manage pigtail catheter placement limit conclusion draw study summary'])\n",
    "\n",
    "\n",
    "\n",
    "bertsum.append(['determinar presena de anticorpos ige especficos para superantgenos estafiloccicos e o grau de sensibilizao mediada por esse assim como se esses esto associado gravidade da asma em pacientes adultos',\n",
    " 'superantigen activity staphylococcal toxin consist direct stimulation class ii mhc receptor t cell independently antigen presentation antigen present cell stimulate proliferation activity t lymphocyte',\n",
    " 'investigate population asthma patient treat university hospital city rio de janeiro brazil have risk factor increase staphylococcal colonization infection order correlate clinical severity asthma presence staphylococcal toxin specific ige antibody degree ige mediate sensitization',\n",
    " 'exclusion criterion follow presence copd atopic dermatitis asthma exacerbation week presence respiratory infection use antimicrobial agent week use systemic corticosteroid therapy seven day week history immunodeficiency neoplasia connective tissue disease kidney failure sinonasal polyposis chronic sinus disease cystic fibrosis bronchiectasis pregnancy smoke month decline participate study write informed consent',\n",
    " 'clement clarke international essex england reference value propose nunn gregg',\n",
    " 'sample size calculate order provide power value p consider statistically significant',\n",
    " 'participate patient give write informed consent treatment provide decline participate study way affect decision',\n",
    " 'median age year year ma group year msa group female white individual having predominate sample mean body mass index bmi kg m',\n",
    " 'patient rhinitis positive skin test result aeroallergen family history atopy history smoking',\n",
    " 'fact patient msa significantly old ma group fact asthma tend severe old individual especially onset asthma occur old age'])\n",
    "\n",
    "\n",
    "\n",
    "bertsum.append(['development human society industrialization change stress source change disease pattern civilized society result disease pattern change traditional disease infectious disease malnutrition disease heart disease diabetes accident forth',\n",
    " 'anxiety cause increase heart rate breathing high blood pressure mortality',\n",
    " 'associate increase risk mortality disability increase medical care functional impairment daily activity clinical depression report patient bypass surgery accord world health organization depression fourth chronic disease disability comparable major chronic disorder',\n",
    " 'nurse patient communication essential element theory accord peplau theory',\n",
    " 'hospitalization complex surgery heart surgery stressful process lead depression patient',\n",
    " 'study approve ethic committee jondi shapoor medical university ahvaz code',\n",
    " 'adjust mean anxiety score suggest anxiety intervention group low control group',\n",
    " 'analysis covariance determine effect peplau therapeutic communication depression level subject',\n",
    " 'accord result peplau therapeutic communication effective reduce anxiety depression patient candidate coronary artery bypass',\n",
    " 'previous study patient suffer anxiety depression month year surgery',\n",
    " 'model enable nurse help patient discharge spend time patient'])\n",
    "\n",
    "bertsum.append(['study report literature lumbar discectomy destandau endospine system',\n",
    " 'endoscopic discectomy carry conventional micro disc surgery instrument minimal invasive route',\n",
    " 'advantage use minimal invasive spinal surgical technique treatment lumbar disc herniation small incision limited tissue disruption enhance visualization well magnification illumination short hospital stay fast recovery posterior spinal endoscopic system disc surgery destandau endospine system foley smith metrx system see viable alternative open disc aim study present result patient operate edoscopic discectomy discuss technical point shorten learning curve',\n",
    " 'consist endospine tube trocar work insert figure b',\n",
    " 'port degree endoscope second suction cannula port big work instrument fourth port dural nerve root retractor',\n",
    " 'final follow patient relieve sciatica satisfied procedure',\n",
    " 'base modify macnab criteria patient excellent good fair poor result',\n",
    " 'nerve root injury encounter try medial facetectomy open recess kerrison rongeur cause severe laceration nerve root nerve root continuity',\n",
    " 'study shin et al case compare med microscopic group md',\n",
    " 'aforementioned author conclude med procedure invasive md cause muscle damage backache',\n",
    " 'endospine system excellent modality address discogenic radiculopathy decompress lumbar canal stenosis'])\n",
    "\n",
    "bertsum.append(['global regional leave ventricular lv function know indicator cardiac disease quantitative value ventricular volume myocardial mass independent predictor morbidity mortality patient coronary artery disease',\n",
    " 'diastolic systolic image easily produce datum set retrospective ecg gate technique obtain lv end diastolic end systolic volume edvs esvs',\n",
    " 'recently observe increase tendency utilize low radiation dose prospective gating coronary angiography limit possibility volumetric assessment ventricular function',\n",
    " 'study semiautomated software quantitative functional analysis lv user define mitral valve plane arbitrary point lv option expand reduce area segmentation',\n",
    " 'relatively high radiation dose result protocol optimize thin slice high resolution imaging coronary artery',\n",
    " 'new development mdct technology allow examination patient high heart rate reduce dose beta blocker presently mdct examination possible contain phase considerably low dose order msv',\n",
    " 'software identify pacemaker wire high density contrast segment pacing wire ventricle result fail segmentation version software study able segment myocardium order quantify lv myocardial mass important certain cardiac disease hypertrophic cardiomyopathy',\n",
    " 'short fluoroscopic resolution currently available option sufficient motion free systolic diastolic cardiac imaging heart rate min additionally use b blocker reduce heart rate bpm influence functional parameter measure beta blocker influence lvef lead underestimation',\n",
    " 'validate consistency result mri lend support use mdct derive result'])\n",
    "\n",
    "\n",
    "bertsum.append(['endometriosis prevalent cause infertility pelvic pain dysmenorrhea dyspareunia reproductive age woman',\n",
    " 'woman age year proven endometriosis laparoscopy vas test score dysmenorrhea pelvic pain second mense operative laparoscopy',\n",
    " 'eligible patient assign simple randomization receive vitamin d placebo vitamin d group d group prescribe oral vitamin d iu weekly week capsule d vigel vitamin iu daana pharma tabriz iran placebo group p group prescribe capsule placebo daana pharma tabriz iran weekly week',\n",
    " 'pair sample t test compare quantitative normal datum treatment group pearson chi square test match compare categorical variable group small sample size',\n",
    " 'double blind randomize clinical trial week laparoscopic treatment endometriosis significant difference effect vitamin cholecalciferol placebo severity dysmenorrhea pelvic pain',\n",
    " 'literature search find study indirectly relate vitamin d endometriosis',\n",
    " 'vitamin d deficiency define serum dihydroxy vitamin level ng ml',\n",
    " 'sample assess exist endometriosis low level serum vitamin d common healthy italian pre menopausal woman result vitamin d effective relieve dysmenorrhea month treatment woman endometriosis woman vitamin d deficiency',\n",
    " 'relationship vitamin d pathogenesis endometriosis study vitamin d effective treatment endometriosis relate pain',\n",
    " 'limitation study high prevalence vitamin d deficiency country worldwide mean high percentage sample vitamin d deficiency affect result research'])\n",
    "\n",
    "\n",
    "bertsum.append(['generally accept microleakage fill material root canal wall adversely affect outcome root canal treatment',\n",
    " 'association gutta percha cone root canal sealer traditionally purpose decade dentin adhesive technology incorporate root canal fill technique reduce apical coronal leakage bond root canal wall',\n",
    " 'reason removal smear layer ethylenediamine tetraacetic acid edta provide etch pattern usually associate hybrid layer consider important factor dentin bonding',\n",
    " 'crown technique k file constant irrigation naocl',\n",
    " 'coronal surface root filling light cure s mw cm',\n",
    " 'tooth immerse aqueous silver nitrate solution darkness buffer naoh n h',\n",
    " 'silver impregnate tooth rinse place photodeveloper h fluorescence light reduce silver ion metallic silver',\n",
    " 'lake bluff il usa longitudinally section isomet precision see buehler low speed rpm water cool diamond blade',\n",
    " 'ed inca software oxford uk england perform low magnification pre determined area m figure high magnification identification silver punctually determine exact location figure',\n",
    " 'replica cover carbon investigate presence gap dentin sealer sealer cone interface describe previously secondary electron se mode',\n",
    " 'gap continuous mm region interface analyze figure classify type interface gap free type gap dentin sealer interface figure type gap sealer cone interface figure type type gap present',\n",
    " 'sealer group ah plus ah primer epiphany type gap consider explanatory variable ah plus type reference'])\n",
    "\n",
    "bertsum.append(['pursue goal improve health literacy require alliance health education sector improve literacy level population',\n",
    " 'caplan propose community theory indicate gap researcher policymaker day gap form researcher user',\n",
    " 'proposal non invasive project approve research committee hamadan medical university january',\n",
    " 'ask parent sample survey participate pretest answer questionnaire kindergarten',\n",
    " 'advantage study comparison evans evans method knowledge transfer bring bear homework assignment study obligation home work parent educate enjoy child singing account fact make communication young child difficult adolescent parent sure knowledge transfer family continuous friendly communication work',\n",
    " 'christensen express importance child role promote family health status article emphasize activity child perform enhance health promote family health situation health effort',\n",
    " 'acceptable result survey publish poem book child republish welcome public'])\n",
    "\n",
    "bertsum.append(['value ketogenic diet kd recognize treatment epilepsy exact mechanism exert effect remain enigma',\n",
    " 'different regular antiepileptic medication aed discover lead use clinical situation epilepsy',\n",
    " 'path neuroprotection modulate decrease generation ros consider relate pufas effect uncouple protein',\n",
    " 'hypothesize mitochondrial abnormality impair ability brain tumor generate energy ketone body unlike normal cell',\n",
    " 'human pilot study animal model study show improvement autistic behavior parameter kd treatment',\n",
    " 'factor crucial application kd medical condition intractable epilepsy inherent difficulty use'])\n",
    "\n",
    "bertsum.append(['endotracheal tube ett place optimal level avoid inadvertent complication',\n",
    " 'hand ett shallow cause vocal cord injury ett balloon accidental extubation',\n",
    " 'purpose study determine surface anatomical landmark predict mid tracheal level adult patient',\n",
    " 'cc cricoid cartilage ett endotracheal tube msj manubriosternal juction mtl level middle trachea ssn suprasternal notch vc vocal cord',\n",
    " 'altman plot show agreement mid tracheal level mtl surface measurement surface anatomical landmark',\n",
    " 'right bias subtraction mid tracheal level surface distance suprasternal notch manubriosternal junction limit agreement mean difference mm'])\n",
    "\n",
    "\n",
    "bertsum.append(['thyroid cancer common endocrine neoplasia account human cancer',\n",
    " 'advance predictive genetic testing ret mutation enable early diagnosis hereditary man syndrome prophylactic thyroidectomy presymptomatic patient prevent mtc',\n",
    " 'intracellular domain consist tyrosine kinase subdomain contain multiple tyrosine residue phosphorylate receptor activation require activation different downstream signal pathway ret',\n",
    " 'activation different downstream signal pathway associate different clinical feature ret mutant thyroid cancer observe syndrome discuss',\n",
    " 'example hirschprung disease congenital disorder neural crest development cause loss function ret mutation',\n",
    " 'subtype account man type case highly penetrant autosomal dominant endocrine tumor syndrome characterize development mtc ret mutation carrier association mtc',\n",
    " 'mtc generally manifestation syndrome develop early childhood usually age age subtype account approximately man type case',\n",
    " 'fmtc consider aggressive clinical variant decrease penetrance delay onset endocrine pathologic manifestation similarly sporadic case familial mtcs isolate associate endocrine tumor',\n",
    " 'american thyroid association guideline task force classify mutation base model use genotypephenotype correlation rank mutation risk level development aggressive mtc low high d table',\n",
    " 'understanding mtc greatly increase discovery ret genotype'])\n",
    "\n",
    "bertsum.append(['accord national regulation diagnostic criterion brain death bd instrumental confirmatory test certain clinical situation intoxication infratentorial process extensive facial damage child young year age case clinical examination inadequate',\n",
    " 'analysis stasis fill dynamic series compute tomographic perfusion ctp provide valuable information interpretation cta result relation phenomenon brain perfusion',\n",
    " 'demographic epidemiological characteristic bd patient present table clinical characteristic bd patientsnumbersexage arterystasis fill ischemic stroke ce cerebral edema aca anterior cerebral artery mca middle cerebral artery pca posterior cerebral artery demographic clinical characteristic bd patient ischemic stroke ce cerebral edema aca anterior cerebral artery mca middle cerebral artery pca posterior cerebral artery control group consist patient undergo surgical clipping intracranial aneurysm',\n",
    " 'cta aortocervical angiography examination bd patient perform methodology describe previously',\n",
    " 'siemens sensation siemens ag erlangen germany perform cta',\n",
    " 'injection ml iodinated contrast medium flow rate ml s',\n",
    " 'cta carry phase manually program fix delay s contrast injection',\n",
    " 'calculation perform osirix software pixmeo sarl bernex',\n",
    " 'choose cerebral artery post processing calculation tdcs',\n",
    " 'whitney test analysis difference group distribution quantitative variable significantly different normal distribution p shapiro',\n",
    " 'observe high enhancement bd patient s table',\n",
    " 'cause mainly altered cerebral autoregulation mechanism sufficient preserve constant cbf cpp mmhg',\n",
    " 'tdcs cerebral artery bd patient characterize significantly long time peak median s compare control s',\n",
    " 'intracranial peak enhancement occur median delay s extracranial peak'])\n",
    "\n",
    "bertsum.append(['study approve institutional ethics committee informed consent obtain patient accordance tenet declaration helsinki',\n",
    " 'eligible candidate undergo relex smile procedure standard surgeon visumax fs laser',\n",
    " 'tissue culture medium commercially available medium semen processing reproductive biology cryocell india pvt ltd new delhi india',\n",
    " 'add slowly final volume ml freeze solution contain human tissue culture medium dmso',\n",
    " 'lenticule transfer petri dish wash twice balanced salt solution minute remove cryoprotectant agent',\n",
    " 'incision open seibel spatula plane pocket dissect',\n",
    " 'depth m choose implantation patient uncertainty refractive outcome novelty nomograms surgeon adequate tissue cap later enhancement surface ablation require',\n",
    " 'patient examine postoperatively day month',\n",
    " 'distance edge lenticule limbus measure point verify visit check center shift position',\n",
    " 'total hoa mm pupil size n ecd schirmer score post fili n',\n",
    " 'potential advantage cryopreservation refractive lenticule largely describe use patient event future ectasia presbyopia restore corneal increase number eye undergo relex smile myopia extract lenticule product novel idea preserve lenticule long term basis cryopreservation potential future use research',\n",
    " 'et d lenticule implant achieve correction aphakia high residual refraction possible explanation result'])\n",
    "\n",
    "bertsum.append(['sepsis major cause death intensive care unit despite improvement antibiotic treatment supportive technique mortality septic shock increase approximately',\n",
    " 'purpose study compare prognostic value biomarker cytokine versus clinical severity score improve death risk prediction',\n",
    " 'compare characteristic survivor versus nonsurvivor univariate analysis receiver operating characteristic roc curve evaluate prognostic value biomarker cytokine predict day mortality variable p value univariate analysis enter multivariate logistic regression analysis identify independent predictor day mortality',\n",
    " 'serum level patient nonsurvival group significantly high survival group day decrease survival group increase nonsurvival group',\n",
    " 'pct normally produce c cell thyroid gland plasma pct level healthy human approximately pg ml normal state half time hour serum',\n",
    " 'study christophe coworker find serum pct day significantly high patient septic shock',\n",
    " 'research demonstrate elevated serum not pro bnp value represent independent predictor poor icu outcome presence clinical severity score cut admission not pro bnp well predict outcome pg ml',\n",
    " 'report crp level severe sepsis low sepsis suggest crp level reflect severity sepsis important proinflammatory anti inflammatory cytokine sepsis study serum level patient nonsurvival group significantly high survival group day',\n",
    " 'mechanism not pro bnp release sepsis complex kinetic characteristic unknown rise quickly peak hour maintain short time',\n",
    " 'course exact role biomarker cytokine sepsis process clear need study'])\n",
    "\n",
    "bertsum.append(['endotracheal intubation important skill anesthesiologist secure airway general anesthesia resuscitation',\n",
    " 'approval research protocol local hospital ethic committee human study obtain personal inform consent american society anesthesiologist asa grade ii adult patient undergo elective surgery receive general anesthesia include study',\n",
    " 'continuous datum express mean standard deviation sd categorical datum express number occurrence percent',\n",
    " 'thickness anterior neck soft tissue measure great difficult laryngoscopy group compare easy laryngoscopy group level hyoid bone thyrohyoid membrane anterior commissure',\n",
    " 'roc curves figure draw medcalc software laryngoscopy grade ii threshold difficult laryngoscopy determine youden index optimal cutoff value sensitivity specificity parentheses mms iig tmd dshb dsem dsac predict difficult laryngoscopy cm cm cm cm cm respectively',\n",
    " 'difference aucs tmd iig area reference line identify suggest mms dshb dsem dsac well tmd iig predict difficult laryngoscopy table'])\n",
    "\n",
    "bertsum.append(['multipath signal occur numerous microwave rf application unwanted portion original transmission propagate alternate path ultimately couple receiver distort amplitude phase desire signal',\n",
    " 'potential interference multipath signal increase substantially near field application figure especially situation receiving transmit hardware integrate',\n",
    " 'early empirical test indicate reflection tank wall impact desire signal array antenna mount cm diameter circle',\n",
    " 'excitation complex propagation characteristic material interface planar cylindrically shape structure previously study depth',\n",
    " 'surface mode consider involve propagation interface tank base couple liquid follow analysis stratton figure show plane wave region impinge interface x region case',\n",
    " 'finding principle signal corruption observe figure reflection tank floor appear figure antenna position base figure',\n",
    " 'likely multipath signal figure short feed line result surface wave travel outside coaxial line plexiglas liquid plexigla air interface outside receiver coaxial feed',\n",
    " 'theoretical consideration section indicate attenuation coaxial line far substantial planar tank base surface wave mode'])\n",
    "\n",
    "bertsum.append(['enucleation generally induce bone regeneration cystic lesion cm diameter eliminate lesion difficult postoperatively predict lesion recurrence bone regeneration case large cyst reason',\n",
    " 'review chart patient visit department oral maxillofacial surgery chosun university dental hospital gwangju korea undergo enucleation decompression',\n",
    " 'patient exclude group review radiological datum cure soft tissue failure maintain window observe month enucleation',\n",
    " 'treatment patient educate wear remove device instruct clean device twice day morning night saline solution',\n",
    " 'panoramic radiograph take month comparison baseline panoramic radiograph cyst enucleation conduct size lesion determine long decrease',\n",
    " 'total follow period year patient suspect recurrence',\n",
    " 'base result august et report histological change likely appear decompression maintain long month propose decompression period month'])\n",
    "\n",
    "bertsum.append(['song complex learn motor skill involve precise coordination vocal respiratory musculature order produce highly stereotyped rendition memorize song model',\n",
    " 'mediate mixture n methyl d aspartate nmda isoxazolepropionic type receptor mediate exclusively nmda type receptor',\n",
    " 'event peak amplitude mv high rise time ms detect automatically result analyze origin pro',\n",
    " 'steady state spike rate estimate count number spike second result plot versus intensity inject current f relationship',\n",
    " 'nmda application hardly change f slope ra projection neuron matter bic present figure p',\n",
    " 'result indicate ampa receptor dependent excitability play crucial role gain modulation nmda',\n",
    " 'report fundamental frequency syllable shift stimulate lman ra pathway mediate nmda receptor',\n",
    " 'ra projection neuron receive input area hvc lman excitatory glutamatergic distinct postsynaptic property',\n",
    " 'ampa receptor mediate excitability derive hvc'])\n",
    "\n",
    "bertsum.append(['stroke rare child common cause neurological disease major cause death rank pediatric document incidence report child',\n",
    " 'multiple opening dura dural patchwas place suture case intra cerebral pressure monitor probe position epidural space bone margin',\n",
    " 'glasgow outcome scale pediatric cerebral performance category scale table calculate month discharge',\n",
    " 'mean follow period month pcpcs patient measure evaluate postoperative neurological outcome',\n",
    " 'appropriate acute cerebral infarction hemorrhagic transformation significant mid line shift',\n",
    " 'brain ct time post operative day reveal sustained brain swelling midline shift note anymore',\n",
    " 'bone flap secure complication end follow',\n",
    " 'article decompressive craniotomy regard emerge procedure save life pediatric stroke guideline craniectomy point fact',\n",
    " 'case pediatric patient infectious disease central nerve system decompressive craniectomy consider life save treatment option'])\n",
    "\n",
    "bertsum.append(['intramammary infection mastitis common reason use antimicrobial dairy cow',\n",
    " 'concept evidence base medicine introduce veterinary medicine apply treatment mastitis impact public health',\n",
    " 'antimicrobial treatment dairy cow create residue milk residue avoidance important aspect mastitis treatment',\n",
    " 'big problem widespread resistance staphylococci particularly staphylococcus aureus penicillin g',\n",
    " 'cure rate mastitis cause penicillin resistant strain aureus inferior mastitis penicillin susceptible strain',\n",
    " 'efficacy imm treatment vary accord causative pathogen good therapeutic response show mastitis cause streptococcus coagulase negative staphylococci corynebacterium spp target antimicrobial therapy clinical mastitis different pathogen systemic route administration suggest efficient imm treatment clinical mastitis antimicrobial theoretically well penetration udder tissue route'])\n",
    "\n",
    "\n",
    "bertsum.append(['chondroblastoma account primary bone tumor usually occur young adolescent',\n",
    " 'local examination reveal swell upper end humerus measure cm cm',\n",
    " 'fnac perform nonaspiration technique patient gauge needle second case ultrasound guide aspiration lesion deep seated',\n",
    " 'histopathology case show sheet polygonal cell thick cell membrane fine pale vacuolate cytoplasm',\n",
    " 'past decade witness impressive use fnac diagnosis bone tumor despite limited case series describe fnac finding chondroblastoma'])\n",
    "\n",
    "bertsum.append(['amphotericin b amb amphoteric polyene macrolide consider main option antifungal therapy life threaten mycotic infection year despite introduction new antifungal medication azole voriconazole echinocandin caspofungin well safety profile',\n",
    " 'major feature amb induce nephrotoxicity include increase serum creatinine level decrease glomerular filtration rate gfr urinary potassium wasting hypokalemia urinary magnesium wasting hypomagnesemia',\n",
    " 'purpose investigation specifically assess frequency time onset possible associate factor amb nephrotoxicity hospitalize patient hematology oncology ward southwest iran',\n",
    " 'continuous datum express mean standard deviation sd mean standard error se',\n",
    " 'fisher s exact test apply category expect frequency',\n",
    " 'parametric non parametric continuous variable examine independent mann whitney u test respectively stepwise method',\n",
    " 'statistically significant difference duration hospitalization patient amb nephrotoxicity day respectively p',\n",
    " 'mortality rate comparable patient amb nephrotoxicity p',\n",
    " 'mean sd onset hypokalemia amb administration day',\n",
    " 'wide variation incidence amb nephrotoxicity note different study methodology clinical setting relevant risk factor definition',\n",
    " 'amb nephrotoxicity resolve spontaneously half affected patient intervention',\n",
    " 'mortality duration hospitalization comparable patient amb nephrotoxicity'])\n",
    "\n",
    "bertsum.append(['o glycan find play important role protein stability tertiary structure stabilize protein conformation modulate activity enzyme reversible attachment o link glcnac cytoplasmic nuclear protein signal molecule essential number recognition process sort determinant guide modify protein cell place biosynthesis target location',\n",
    " 'alditol acetate obtain acid hydrolysis reduction sodium borodeuteride peracetylation hydrolysis peracetylation exist alditol identify gc ms',\n",
    " 'analysis core trisaccharide alditol lusitanicus select ion chromatogram m z m z partially methylate alditol acetate obtain permethylation h methyliodide hydrolysis reduction sodium borohydride peracetylation b ei ms spectrum fragmentation pattern tri o methyl galnac ol c tetra o methyl galacitol',\n",
    " 'structure plot generate notation consortium functional glycomic visual editor glycoworkbench',\n",
    " 'software application develop available eurocarbdb project',\n",
    " 'fourth structure m z obtain sufficient amount gc ms consist core trisaccharide elongate unmethylate hexose arm methylated hexose fig',\n",
    " 'overview structure table o glycan distribution select snail specie give total o glycan',\n",
    " 'n glycosidase pngase f remove n glycan completely mild condition able release residual n glycan result n glycan impurity o glycan fraction',\n",
    " 'monosaccharide constituent galnac gal man fuc building block structure',\n",
    " 'frequently elongate o gal residue elongation trisaccharide core hexose methyl group appear snail specie elongation fucosylation rare event'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98403e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b90afc05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "--- 35.996602058410645 seconds ---\n"
     ]
    }
   ],
   "source": [
    "bertsum_ROUGE_1 = []\n",
    "bertsum_ROUGE_2 = []\n",
    "bertsum_ROUGE_3 = []\n",
    "\n",
    "bertsum_BLEU_1 = []\n",
    "bertsum_BLEU_2 = []\n",
    "bertsum_BLEU_3 = []\n",
    "\n",
    "bertsum_F1_1 = []\n",
    "bertsum_F1_2 = []\n",
    "bertsum_F1_3 = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(bertsum)):  #len(bertsum)\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation \n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "    \n",
    "    gen_summary_n = n_grams(bertsum[i])\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    bertsum_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    bertsum_BLEU_1.append(metrics_1['BLEU'])\n",
    "    bertsum_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    bertsum_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    bertsum_BLEU_2.append(metrics_2['BLEU'])\n",
    "    bertsum_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    bertsum_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    bertsum_BLEU_3.append(metrics_3['BLEU'])\n",
    "    bertsum_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "bertsum_df = pd.DataFrame({'ROUGE_1' : bertsum_ROUGE_1, 'ROUGE_2' : bertsum_ROUGE_2, 'ROUGE_3' : bertsum_ROUGE_3, \n",
    "                            'BLEU_1' : bertsum_BLEU_1, 'BLEU_2' : bertsum_BLEU_2, 'BLEU_3' : bertsum_BLEU_3, \n",
    "                            'F1_1' : bertsum_F1_1, 'F1_2' : bertsum_F1_2, 'F1_3': bertsum_F1_3})\n",
    "\n",
    "bertsum_df.to_csv(\"../data/bertsum_fil.csv\", index = False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1daff5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\". \".join(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6cba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d327f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e13bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef0c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
