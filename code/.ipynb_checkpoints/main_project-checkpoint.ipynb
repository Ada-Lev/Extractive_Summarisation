{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60c71d3",
   "metadata": {},
   "source": [
    "# Import data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cc7f9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast # for conversin of a string representation of a list to a list\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') # Spacy language model\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx # for PageRank\n",
    "    \n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "test_df.drop([8, 16, 28, 38, 39], axis = 0, inplace = True)\n",
    "test_df.reset_index(inplace = True)\n",
    "test_df = test_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f0b07",
   "metadata": {},
   "source": [
    "# Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a584eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    \"\"\"Return ROUGE, BLEU, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        result: dictionary with ROUGE, BLEU, and F1 score\n",
    "    \"\"\"\n",
    "    freq_ROUGE = 0\n",
    "    for each in gold:\n",
    "        if each in pred:\n",
    "            freq_ROUGE += 1\n",
    "    # portion of the words from golden summary appering in the generated summary\n",
    "    ROUGE = freq_ROUGE/len(gold) \n",
    "    #print(\"--------------ROUGE (Recall):\")\n",
    "    #print(f\"{round(ROUGE*100, 2)}%\")\n",
    "    \n",
    "    # Brevity penalised frequency\n",
    "    pred_count = {}\n",
    "    for each in set(pred):\n",
    "        pred_count[each] = min(gold.count(each), pred.count(each))\n",
    "\n",
    "    freq_BLEU = sum(pred_count.values())\n",
    "    \n",
    "    # (with brevity penalty) portion of the words from generated summary appering in the generated summary\n",
    "    BLEU = freq_BLEU/len(pred) \n",
    "    #print(\"--------------BLEU (Precision):\")\n",
    "    #print(f\"{round(BLEU*100, 2)}%\")\n",
    "\n",
    "    if ROUGE == 0 or BLEU == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*(ROUGE * BLEU)/(ROUGE + BLEU)\n",
    "    #print(\"--------------F1 score:\")\n",
    "    #print(f\"{round(f1*100, 2)}%\")\n",
    "    \n",
    "    return {'ROUGE' : round(ROUGE*100, 2), 'BLEU' : round(BLEU*100, 2), 'F1' : round(f1*100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56c209",
   "metadata": {},
   "source": [
    "# Text and summary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ab92ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text = test_df['article_text'][0], summary = test_df['abstract_text'][0]):\n",
    "    \"\"\"Return text and summary before and after preprocessing \n",
    "    (stop word removal, non-alphabetic characters removal, and lemmatisation).\n",
    "    \n",
    "    Args:\n",
    "        text: String representaion of a list of sentences in the article body.\n",
    "        summary: String representaion of a list of sentences of a human-written abstract.\n",
    "    \n",
    "    Returns:\n",
    "        dictionary with text, summary, preprocessed text, and preprocessed summary.\n",
    "    \"\"\"\n",
    "    text = ast.literal_eval(text) # Convert sttring representatino of a list to a list\n",
    "    summary = ast.literal_eval(summary)\n",
    "\n",
    "    # Remove html tags\n",
    "    \n",
    "    summary = [sent.replace(\" </S>\", \"\") for sent in summary] \n",
    "    summary = [sent.replace(\"<S> \", \"\") for sent in summary]\n",
    "\n",
    "    # Text preprocessing\n",
    "    \n",
    "    text_prep = []\n",
    "    for sent in text:\n",
    "        doc = nlp(sent)\n",
    "        doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "        doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "        text_prep.append(\" \".join(doc))\n",
    "\n",
    "    # Abstract preprocessing\n",
    "\n",
    "    summary_prep = []\n",
    "    for sent in summary:\n",
    "        doc = nlp(sent)\n",
    "        doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "        doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "        summary_prep.append(\" \".join(doc))\n",
    "        \n",
    "    return {'text' : text, 'summary' : summary, 'text_prep' : text_prep, 'summary_prep' : summary_prep}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048dead",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0545062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(sentences):\n",
    "    \"\"\"TextRank algorithm. Rank sentences using PageRank \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = {}\n",
    "\n",
    "    for ind in range(len(sentences)): \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec[ind] = bow\n",
    "        \n",
    "    sim_mat = np.zeros((len(sentences), len(sentences)))\n",
    "    for ind_1 in range(len(sentences)):\n",
    "        for ind_2 in range(ind_1, len(sentences)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f0090",
   "metadata": {},
   "source": [
    "# WordRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1da74b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordrank(sentences):\n",
    "    \"\"\"WordRank algorithm. Rank sentences using PageRank \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words)) # Find unique words in all processed sentences\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    # Find sentence scores as sum of all included word scores\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48119472",
   "metadata": {},
   "source": [
    "# Hybrid64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc8b001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid64(sentences):\n",
    "    \"\"\"Hybrid WordRank+TextRank algorithm. Rank sentences using PageRank. \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Retrieve important sentences indices\n",
    "\n",
    "    new_sent_ind = []\n",
    "    count = 0\n",
    "\n",
    "    for k in sorted_scores.keys():\n",
    "        if count > len(sorted_scores)*0.64:\n",
    "            break\n",
    "        else:\n",
    "            new_sent_ind.append(k)\n",
    "            count += 1\n",
    "            \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = []\n",
    "\n",
    "    for ind in new_sent_ind: \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec.append(bow) # reindexed. old index can be found via new_sent_ind[this index]\n",
    "        \n",
    "    sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "    for ind_1 in range(len(new_sent_ind)):\n",
    "        for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "    \n",
    "    # Need to return to old indices. Redefine keys of this dictionary:\n",
    "    new_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        new_scores[new_sent_ind[k]] = v\n",
    "\n",
    "    sorted_scores = dict(sorted(new_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627ad66",
   "metadata": {},
   "source": [
    "# Hybrid80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5936d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid80(sentences):\n",
    "    \"\"\"Hybrid WordRank+TextRank algorithm. Rank sentences using PageRank. \n",
    "    \n",
    "    Args:\n",
    "        sentences: List of preprocessed sentences to rank.\n",
    "    \n",
    "    Returns:\n",
    "        sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        [words.append(word) for word in sent.split()]\n",
    "\n",
    "    words = list(set(words))\n",
    "\n",
    "    # Calculate the cosine similarity -- store result in a similarity matrix\n",
    "    sim_mat = np.zeros((len(words), len(words)))\n",
    "    for ind_1 in range(len(words)):\n",
    "        for ind_2 in range(ind_1, len(words)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "    sent_scores = {}\n",
    "\n",
    "    for ind in range(len(sentences)):\n",
    "        w_score = 0\n",
    "        for word in sentences[ind].split():\n",
    "            w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "        sent_scores[ind] = w_score\n",
    "\n",
    "    sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Retrieve important sentences indices\n",
    "\n",
    "    new_sent_ind = []\n",
    "    count = 0\n",
    "\n",
    "    for k in sorted_scores.keys():\n",
    "        if count > len(sorted_scores)*0.8:\n",
    "            break\n",
    "        else:\n",
    "            new_sent_ind.append(k)\n",
    "            count += 1\n",
    "            \n",
    "    # Vectorise sentences. Use Spacy embeddings and create bag of words vectors via summation.\n",
    "    sent_vec = []\n",
    "\n",
    "    for ind in new_sent_ind: \n",
    "        bow = np.zeros((1, 300))\n",
    "        for word in sentences[ind].split():\n",
    "            bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "        sent_vec.append(bow) # reindexed. old index can be found via new_sent_ind[this index]\n",
    "        \n",
    "    sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "    for ind_1 in range(len(new_sent_ind)):\n",
    "        for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "            sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "            sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "    sim_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "    \n",
    "    # Need to return to old indices. Redefine keys of this dictionary:\n",
    "    new_scores = {}\n",
    "    for k, v in scores.items():\n",
    "        new_scores[new_sent_ind[k]] = v\n",
    "\n",
    "    sorted_scores = dict(sorted(new_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57145ac5",
   "metadata": {},
   "source": [
    "# Generate summary based on the ranked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "54f08e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sorted_scores, text, summary, text_prep):\n",
    "    \"\"\"Generate summary based on the top ranked sentences\n",
    "    \n",
    "    Args:\n",
    "        text_prep: List of preprocessed sentences to rank\n",
    "        text: List of initial sentences to rank\n",
    "        summary: golden summary\n",
    "        sorted_scores: sorted dictionary with keys corresponding to initial sentences indices \n",
    "        in the provided list and values as scores.\n",
    "    \n",
    "    Returns:\n",
    "        Generated summaries: with and wirhout preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    top_sent = []\n",
    "    top_sent_prep = []\n",
    "    for k, v in sorted_scores.items():\n",
    "        # Number of sentences in generated summary equal to golden summary length\n",
    "        if len(top_sent_prep) < len(summary): \n",
    "            top_sent.append(text[k])\n",
    "            top_sent_prep.append(text_prep[k])\n",
    "        else:\n",
    "            break\n",
    "    return {'gen_summary' : top_sent, 'gen_summary_prep' : top_sent_prep}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8217b",
   "metadata": {},
   "source": [
    "# Obtain unigrams, bigrams, and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4eca70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text):\n",
    "    \"\"\"Obtain unigrams, bigrams, and trigrams in the given text\n",
    "    \n",
    "    Args:\n",
    "        text: List of sentences\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with unigrams, bigrams, and trigrams\n",
    "    \"\"\"\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    #Unigrams\n",
    "    text_1 = text.split()\n",
    "\n",
    "    # Bigrams\n",
    "    text_2 = []\n",
    "\n",
    "    for i in range(len(text_1) - 1):\n",
    "        text_2.append(text_1[i] + \" \" + text_1[i + 1])\n",
    "\n",
    "    # Trigrams\n",
    "    text_3 = []\n",
    "\n",
    "    for i in range(len(text_1) - 2):\n",
    "        text_3.append(text_1[i] + \" \" + text_1[i + 1] +  \" \" + text_1[i + 2])\n",
    "    \n",
    "    return {'unigrams' : text_1, 'bigrams' : text_2, 'trigrams' : text_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65145b64",
   "metadata": {},
   "source": [
    "# TextRank algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "424a5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_ROUGE_1 = []\n",
    "textrank_ROUGE_2 = []\n",
    "textrank_ROUGE_3 = []\n",
    "\n",
    "textrank_BLEU_1 = []\n",
    "textrank_BLEU_2 = []\n",
    "textrank_BLEU_3 = []\n",
    "\n",
    "textrank_F1_1 = []\n",
    "textrank_F1_2 = []\n",
    "textrank_F1_3 = []\n",
    "\n",
    "for i in range(test_df.shape[0]):\n",
    "    \n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = textrank(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    textrank_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    textrank_BLEU_1.append(metrics_1['BLEU'])\n",
    "    textrank_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    textrank_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    textrank_BLEU_2.append(metrics_2['BLEU'])\n",
    "    textrank_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    textrank_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    textrank_BLEU_3.append(metrics_3['BLEU'])\n",
    "    textrank_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "textrank_df = pd.DataFrame({'ROUGE_1' : textrank_ROUGE_1, 'ROUGE_2' : textrank_ROUGE_2, 'ROUGE_3' : textrank_ROUGE_3, \n",
    "                            'BLEU_1' : textrank_BLEU_1, 'BLEU_2' : textrank_BLEU_2, 'BLEU_3' : textrank_BLEU_3, \n",
    "                            'F1_1' : textrank_F1_1, 'F1_2' : textrank_F1_2, 'F1_3': textrank_F1_3})\n",
    "\n",
    "textrank_df.to_csv(\"../data/textrank.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6734c6",
   "metadata": {},
   "source": [
    "# WordRank algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "be55722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "wordrank_ROUGE_1 = []\n",
    "wordrank_ROUGE_2 = []\n",
    "wordrank_ROUGE_3 = []\n",
    "\n",
    "wordrank_BLEU_1 = []\n",
    "wordrank_BLEU_2 = []\n",
    "wordrank_BLEU_3 = []\n",
    "\n",
    "wordrank_F1_1 = []\n",
    "wordrank_F1_2 = []\n",
    "wordrank_F1_3 = []\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = wordrank(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    wordrank_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    wordrank_BLEU_1.append(metrics_1['BLEU'])\n",
    "    wordrank_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    wordrank_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    wordrank_BLEU_2.append(metrics_2['BLEU'])\n",
    "    wordrank_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    wordrank_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    wordrank_BLEU_3.append(metrics_3['BLEU'])\n",
    "    wordrank_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "wordrank_df = pd.DataFrame({'ROUGE_1' : wordrank_ROUGE_1, 'ROUGE_2' : wordrank_ROUGE_2, 'ROUGE_3' : wordrank_ROUGE_3, \n",
    "                            'BLEU_1' : wordrank_BLEU_1, 'BLEU_2' : wordrank_BLEU_2, 'BLEU_3' : wordrank_BLEU_3, \n",
    "                            'F1_1' : wordrank_F1_1, 'F1_2' : wordrank_F1_2, 'F1_3': wordrank_F1_3})\n",
    "\n",
    "wordrank_df.to_csv(\"../data/wordrank.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eef040",
   "metadata": {},
   "source": [
    "# Hybrid64 algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c389bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "hybrid64_ROUGE_1 = []\n",
    "hybrid64_ROUGE_2 = []\n",
    "hybrid64_ROUGE_3 = []\n",
    "\n",
    "hybrid64_BLEU_1 = []\n",
    "hybrid64_BLEU_2 = []\n",
    "hybrid64_BLEU_3 = []\n",
    "\n",
    "hybrid64_F1_1 = []\n",
    "hybrid64_F1_2 = []\n",
    "hybrid64_F1_3 = []\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = hybrid64(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    hybrid64_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    hybrid64_BLEU_1.append(metrics_1['BLEU'])\n",
    "    hybrid64_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    hybrid64_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    hybrid64_BLEU_2.append(metrics_2['BLEU'])\n",
    "    hybrid64_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    hybrid64_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    hybrid64_BLEU_3.append(metrics_3['BLEU'])\n",
    "    hybrid64_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "hybrid64_df = pd.DataFrame({'ROUGE_1' : hybrid64_ROUGE_1, 'ROUGE_2' : hybrid64_ROUGE_2, 'ROUGE_3' : hybrid64_ROUGE_3, \n",
    "                            'BLEU_1' : hybrid64_BLEU_1, 'BLEU_2' : hybrid64_BLEU_2, 'BLEU_3' : hybrid64_BLEU_3, \n",
    "                            'F1_1' : hybrid64_F1_1, 'F1_2' : hybrid64_F1_2, 'F1_3': hybrid64_F1_3})\n",
    "\n",
    "hybrid64_df.to_csv(\"../data/hybrid64.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7995fb",
   "metadata": {},
   "source": [
    "# Hybrid80 algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a48c232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "hybrid80_ROUGE_1 = []\n",
    "hybrid80_ROUGE_2 = []\n",
    "hybrid80_ROUGE_3 = []\n",
    "\n",
    "hybrid80_BLEU_1 = []\n",
    "hybrid80_BLEU_2 = []\n",
    "hybrid80_BLEU_3 = []\n",
    "\n",
    "hybrid80_F1_1 = []\n",
    "hybrid80_F1_2 = []\n",
    "hybrid80_F1_3 = []\n",
    "\n",
    "for i in range(test_df.shape[0]): #test_df.shape[0]\n",
    "    print(i)\n",
    "    # Preprocess text including stop word removal, non-alphabetic characters removal, and lemmatisation\n",
    "    prep = preprocess(text = test_df['article_text'][i], summary = test_df['abstract_text'][i])\n",
    "    text, summary, text_prep, summary_prep = prep['text'], prep['summary'], prep['text_prep'], prep['summary_prep'] \n",
    "\n",
    "    # Rank sentences \n",
    "    sorted_scores = hybrid80(text_prep)\n",
    "\n",
    "    # Generate summary based on the ranked sentences\n",
    "    gen = generate_summary(sorted_scores, text, summary, text_prep)\n",
    "    gen_summary, gen_summary_prep = gen['gen_summary'], gen['gen_summary_prep']\n",
    "\n",
    "    # Obtain golden and generated unigrams, bigrams, and trigrams\n",
    "    summary_n = n_grams(summary_prep)\n",
    "    summary_1, summary_2, summary_3 = summary_n['unigrams'], summary_n['bigrams'], summary_n['trigrams']\n",
    "\n",
    "    gen_summary_n = n_grams(gen_summary_prep)\n",
    "    gen_summary_1, gen_summary_2, gen_summary_3 = gen_summary_n['unigrams'], gen_summary_n['bigrams'], gen_summary_n['trigrams']\n",
    "\n",
    "    # Evaluation of unigrams\n",
    "    metrics_1 = evaluation_report(summary_1, gen_summary_1)\n",
    "\n",
    "    hybrid80_ROUGE_1.append(metrics_1['ROUGE'])\n",
    "    hybrid80_BLEU_1.append(metrics_1['BLEU'])\n",
    "    hybrid80_F1_1.append(metrics_1['F1'])\n",
    "\n",
    "    # Evaluation of bigrams\n",
    "    metrics_2 = evaluation_report(summary_2, gen_summary_2)\n",
    "\n",
    "    hybrid80_ROUGE_2.append(metrics_2['ROUGE'])\n",
    "    hybrid80_BLEU_2.append(metrics_2['BLEU'])\n",
    "    hybrid80_F1_2.append(metrics_2['F1'])\n",
    "\n",
    "    # Evaluation of trigrams\n",
    "    metrics_3 = evaluation_report(summary_3, gen_summary_3)\n",
    "\n",
    "    hybrid80_ROUGE_3.append(metrics_3['ROUGE'])\n",
    "    hybrid80_BLEU_3.append(metrics_3['BLEU'])\n",
    "    hybrid80_F1_3.append(metrics_3['F1'])\n",
    "\n",
    "hybrid80_df = pd.DataFrame({'ROUGE_1' : hybrid80_ROUGE_1, 'ROUGE_2' : hybrid80_ROUGE_2, 'ROUGE_3' : hybrid80_ROUGE_3, \n",
    "                            'BLEU_1' : hybrid80_BLEU_1, 'BLEU_2' : hybrid80_BLEU_2, 'BLEU_3' : hybrid80_BLEU_3, \n",
    "                            'F1_1' : hybrid80_F1_1, 'F1_2' : hybrid80_F1_2, 'F1_3': hybrid80_F1_3})\n",
    "\n",
    "hybrid80_df.to_csv(\"../data/hybrid80.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
