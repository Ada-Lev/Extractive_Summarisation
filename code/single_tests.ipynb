{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157b6956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        article_text  \\\n",
      "0  [\"anxiety affects quality of life in those liv...   \n",
      "1  ['small non - coding rnas are transcribed into...   \n",
      "2  ['ohss is a serious complication of ovulation ...   \n",
      "3  ['congenital adrenal hyperplasia ( cah ) refer...   \n",
      "4  ['type 1 diabetes ( t1d ) results from the des...   \n",
      "\n",
      "                                       abstract_text  \n",
      "0  [\"<S> research on the implications of anxiety ...  \n",
      "1  ['<S> small non - coding rnas include sirna , ...  \n",
      "2  ['<S> objective : to evaluate the efficacy and...  \n",
      "3  ['<S> congenital adrenal hyperplasia is a grou...  \n",
      "4  ['<S> objective(s):pentoxifylline is an immuno...  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import ast\n",
    "    \n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c99c67",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76b28dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx # for PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894003b",
   "metadata": {},
   "source": [
    "# Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48d92eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(gold, pred):\n",
    "    \"\"\"Print ROUGE_1, BLEU_1, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        gold: The set with the gold-standard values.\n",
    "        pred: The set with the predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        Nothing, but prints the ROUGE_1, BLEU_1, and F1 values computed\n",
    "        based on the specified sets.\n",
    "    \"\"\"\n",
    "    freq_ROUGE = 0\n",
    "    for each in gold:\n",
    "        if each in pred:\n",
    "            freq_ROUGE += 1\n",
    "    # portion of the words from golden summary appering in the generated summary\n",
    "    ROUGE = freq_ROUGE/len(gold) \n",
    "    print(\"--------------ROUGE (Recall):\")\n",
    "    print(f\"{round(ROUGE*100, 2)}%\")\n",
    "    \n",
    "    # Brevity penalised frequency\n",
    "    pred_count = {}\n",
    "    for each in set(pred):\n",
    "        pred_count[each] = min(gold.count(each), pred.count(each))\n",
    "\n",
    "    freq_BLEU = sum(pred_count.values())\n",
    "    \n",
    "    # (with brevity penalty) portion of the words from generated summary appering in the generated summary\n",
    "    BLEU = freq_BLEU/len(pred) \n",
    "    print(\"--------------BLEU (Precision):\")\n",
    "    print(f\"{round(BLEU*100, 2)}%\")\n",
    "\n",
    "    f1 = 2*(ROUGE * BLEU)/(ROUGE + BLEU)\n",
    "    print(\"--------------F1 score:\")\n",
    "    print(f\"{round(f1*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcf8d3",
   "metadata": {},
   "source": [
    "# Preprocessing -- take one case as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "103a8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ast.literal_eval(test_df['article_text'][20]) # Convert sttring representatino of a list to a list\n",
    "abstract = ast.literal_eval(test_df['abstract_text'][20])\n",
    "\n",
    "abstract = [sent.replace(\" </S>\", \"\") for sent in abstract]\n",
    "abstract = [sent.replace(\"<S> \", \"\") for sent in abstract]\n",
    "\n",
    "sentences = []\n",
    "for sent in text:\n",
    "    doc = nlp(sent)\n",
    "    doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "    doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "    sentences.append(\" \".join(doc))\n",
    "    \n",
    "corpus = sentences\n",
    "\n",
    "# Abstract preprocessing\n",
    "\n",
    "abstract_sentences_pro = []\n",
    "for sent in abstract:\n",
    "    doc = nlp(sent)\n",
    "    doc = [token.lemma_ for token in doc if token.is_stop == False] # stop word removal and lemmatisation\"\n",
    "    doc = [token for token in doc if token.isalpha() == True] # exlude non-alphabetic lemmas\n",
    "    abstract_sentences_pro.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94adc0",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5633afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (76, 599)\n",
      "18\n",
      "54\n",
      "11\n",
      "41\n",
      "\n",
      "-------------Generated summary:\n",
      "['therefore , it was our objective to assess the potential antimicrobial activity of aristolochia bracteolata using a bioassay - guided fractionation , in order to produce pure compound that can act as the lead compound in developing new , safe , and effective drug to replace the use of the harmful crude plant material .', 'the resulting fractions were tested for antibacterial and antifungal activities . the crude extract and chloroform fraction were significantly active against sea urchin - derived bacillus sp . and both standard strain and clinical isolates of moraxella catarrhalis and were moderately active against s. aureus , b. subtilis , and ps .', 'organic solvent extracts of the plant showed antibacterial activities while the water extract showed antifungal activity .', 'the sterile paper discs ( 6  mm in diameter ) which were impregnated with the plant extract ( 14  mg ) and pure compound ( 10100  g ) were placed aseptically over the bacterial culture on nutrient agar plates .']\n",
      "\n",
      "-------------Real abstract:\n",
      "['a bioassay - guided fractionation of methanol extract of aristolochia bracteolata whole plant was carried out in order to evaluate its antimicrobial activity and to identify the active compounds in this extract .', 'antibacterial and antifungal activities of methanol extract against gram - positive , gram - negative , and fungal strains were investigated by the agar disk diffusion method . among the strains tested , moraxella catarrhalis and sea urchin - derived bacillus sp .', 'showed the highest sensitivity towards the methanol extract and hence they are used as test organisms for the bioassay - guided fractionation . from this', 'extract , aristolochic acid 1 ( aa-1 ) has been isolated and has showed the greatest antibacterial activity against both standard strain and clinical isolates of moraxella catarrhalis with equal minimum inhibitory concentration ( mic ) and minimum bactericidal concentration ( mbc ) values of 25 and 50  g / ml . modification of the aa-1 to aa-1 methyl ester completely abolished the antibacterial activity of the compound and the piperonylic acid moiety of aa-1 which suggested that the coexistence of phenanthrene ring and free carboxylic acid is essential for aa-1 antibacterial activity .']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "# Calculate the cosine similarity -- store result in a similarity matrix\n",
    "sim_mat = np.zeros((X.shape[0], X.shape[0]))\n",
    "for ind_1 in range(X.shape[0]):\n",
    "    for ind_2 in range(ind_1, X.shape[0]):\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(X[ind_1, :], X[ind_2, :])\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "            \n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph)\n",
    "\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "top_sent_pre = [] # before preprocessing\n",
    "top_sent_pro = [] # after preprocessing\n",
    "for k, v in sorted_scores.items():\n",
    "    if len(top_sent_pre) < len(abstract):\n",
    "        print(k)\n",
    "        top_sent_pre.append(text[k])\n",
    "        top_sent_pro.append(sentences[k])\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(\"\\n-------------Generated summary:\")\n",
    "print(top_sent_pre)\n",
    "\n",
    "print(\"\\n-------------Real abstract:\")\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2f740",
   "metadata": {},
   "source": [
    "TextRank features longer sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceda686",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af221dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of unigrams:\n",
      "--------------ROUGE (Recall):\n",
      "51.52%\n",
      "--------------BLEU (Precision):\n",
      "43.68%\n",
      "--------------F1 score:\n",
      "47.27%\n",
      "\n",
      "Evaluation of bigrams:\n",
      "--------------ROUGE (Recall):\n",
      "21.43%\n",
      "--------------BLEU (Precision):\n",
      "18.6%\n",
      "--------------F1 score:\n",
      "19.92%\n",
      "\n",
      "Evaluation of trigrams:\n",
      "--------------ROUGE (Recall):\n",
      "10.31%\n",
      "--------------BLEU (Precision):\n",
      "10.59%\n",
      "--------------F1 score:\n",
      "10.45%\n"
     ]
    }
   ],
   "source": [
    "gen_abst = \" \".join(top_sent_pro)\n",
    "gol_abst = \" \".join(abstract_sentences_pro)\n",
    "\n",
    "# Unigrams\n",
    "\n",
    "gen_abst_1 = gen_abst.split()\n",
    "gol_abst_1 = gol_abst.split()\n",
    "\n",
    "print(\"\\nEvaluation of unigrams:\")\n",
    "evaluation_report(gol_abst_1, gen_abst_1)\n",
    "\n",
    "# Bigrams\n",
    "\n",
    "gen_abst_2 = []\n",
    "gol_abst_2 = []\n",
    "\n",
    "for i in range(len(gol_abst_1) - 1):\n",
    "    gol_abst_2.append(gol_abst_1[i] + \" \" + gol_abst_1[i + 1])\n",
    "for i in range(len(gen_abst_1) - 1):\n",
    "    gen_abst_2.append(gen_abst_1[i] + \" \" + gen_abst_1[i + 1])\n",
    "    \n",
    "print(\"\\nEvaluation of bigrams:\")\n",
    "evaluation_report(gol_abst_2, gen_abst_2)\n",
    "\n",
    "# Trigrams\n",
    "# Trigrams\n",
    "\n",
    "gen_abst_3 = []\n",
    "gol_abst_3 = []\n",
    "\n",
    "for i in range(len(gol_abst_1) - 2):\n",
    "    gol_abst_3.append(gol_abst_1[i] + \" \" + gol_abst_1[i + 1] +  \" \" + gol_abst_1[i + 2])\n",
    "for i in range(len(gen_abst_1) - 2):\n",
    "    gen_abst_3.append(gen_abst_1[i] + \" \" + gen_abst_1[i + 1] +  \" \" + gen_abst_1[i + 2])\n",
    "    \n",
    "print(\"\\nEvaluation of trigrams:\")\n",
    "evaluation_report(gol_abst_3, gen_abst_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae54403",
   "metadata": {},
   "source": [
    "# WordRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0968403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sent in sentences:\n",
    "    [words.append(word) for word in sent.split()]\n",
    "    \n",
    "words = list(set(words)) # Find unique words in all processed sentences\n",
    "\n",
    "# Calculate the cosine similarity -- store result in a similarity matrix\n",
    "sim_mat = np.zeros((len(words), len(words)))\n",
    "for ind_1 in range(len(words)):\n",
    "    for ind_2 in range(ind_1, len(words)):\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "            \n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "# Find sentence scores as sum of all included word scores\n",
    "sent_scores = {}\n",
    "\n",
    "for ind in range(len(sentences)):\n",
    "    w_score = 0\n",
    "    for word in sentences[ind].split():\n",
    "        w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "    sent_scores[ind] = w_score\n",
    "    \n",
    "sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "top_sent_word_pre = []\n",
    "top_sent_word_pro = []\n",
    "for k, v in sorted_scores.items():\n",
    "    if len(top_sent_word_pre) < len(abstract):\n",
    "        top_sent_word_pre.append(text[k])\n",
    "        top_sent_word_pro.append(sentences[k])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549ba7c",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77c5a93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of unigrams:\n",
      "--------------ROUGE (Recall):\n",
      "35.35%\n",
      "--------------BLEU (Precision):\n",
      "14.01%\n",
      "--------------F1 score:\n",
      "20.07%\n",
      "\n",
      "Evaluation of bigrams:\n",
      "--------------ROUGE (Recall):\n",
      "1.02%\n",
      "--------------BLEU (Precision):\n",
      "1.16%\n",
      "--------------F1 score:\n",
      "1.09%\n",
      "\n",
      "Evaluation of trigrams:\n",
      "--------------ROUGE (Recall):\n",
      "2.06%\n",
      "--------------BLEU (Precision):\n",
      "0.65%\n",
      "--------------F1 score:\n",
      "0.98%\n"
     ]
    }
   ],
   "source": [
    "gen_abst_word = \" \".join(top_sent_word_pro)\n",
    "\n",
    "# Unigrams\n",
    "\n",
    "gen_abst_word_1 = gen_abst_word.split()\n",
    "\n",
    "print(\"\\nEvaluation of unigrams:\")\n",
    "evaluation_report(gol_abst_1, gen_abst_word_1)\n",
    "\n",
    "# Bigrams\n",
    "gen_abst_word_2 = []\n",
    "\n",
    "for i in range(len(gen_abst_1) - 1):\n",
    "    gen_abst_word_2.append(gen_abst_word_1[i] + \" \" + gen_abst_word_1[i + 1])\n",
    "    \n",
    "print(\"\\nEvaluation of bigrams:\")\n",
    "evaluation_report(gol_abst_2, gen_abst_word_2)\n",
    "\n",
    "# Trigrams\n",
    "gen_abst_word_3 = []\n",
    "\n",
    "for i in range(len(gen_abst_word_1) - 2):\n",
    "    gen_abst_word_3.append(gen_abst_word_1[i] + \" \" + gen_abst_word_1[i + 1] +  \" \" + gen_abst_word_1[i + 2])\n",
    "    \n",
    "print(\"\\nEvaluation of trigrams:\")\n",
    "evaluation_report(gol_abst_3, gen_abst_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a55b0",
   "metadata": {},
   "source": [
    "# Rerun TextRank with embegginds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4eafc",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d745bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec = {}\n",
    "\n",
    "for ind in range(len(sentences)):\n",
    "    bow = np.zeros((1, 300))\n",
    "    for word in sentences[ind].split():\n",
    "        bow += nlp.vocab[word].vector.reshape(1, 300)\n",
    "    sent_vec[ind] = bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9760b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = np.zeros((len(sentences), len(sentences)))\n",
    "for ind_1 in range(len(sentences)):\n",
    "    for ind_2 in range(ind_1, len(sentences)):\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(sent_vec[ind_1], sent_vec[ind_2])\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "\n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "top_sent_emb_pre = []\n",
    "top_sent_emb_pro = []\n",
    "for k, v in sorted_scores.items():\n",
    "    if len(top_sent_emb_pre) < len(abstract):\n",
    "        top_sent_emb_pre.append(text[k])\n",
    "        top_sent_emb_pro.append(sentences[k])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615f88d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa92a016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of unigrams:\n",
      "--------------ROUGE (Recall):\n",
      "72.73%\n",
      "--------------BLEU (Precision):\n",
      "49.55%\n",
      "--------------F1 score:\n",
      "58.94%\n",
      "\n",
      "Evaluation of bigrams:\n",
      "--------------ROUGE (Recall):\n",
      "35.71%\n",
      "--------------BLEU (Precision):\n",
      "27.27%\n",
      "--------------F1 score:\n",
      "30.93%\n",
      "\n",
      "Evaluation of trigrams:\n",
      "--------------ROUGE (Recall):\n",
      "21.65%\n",
      "--------------BLEU (Precision):\n",
      "18.35%\n",
      "--------------F1 score:\n",
      "19.86%\n"
     ]
    }
   ],
   "source": [
    "gen_abst_emb = \" \".join(top_sent_emb_pro)\n",
    "\n",
    "# Unigrams\n",
    "\n",
    "gen_abst_emb_1 = gen_abst_emb.split()\n",
    "\n",
    "print(\"\\nEvaluation of unigrams:\")\n",
    "evaluation_report(gol_abst_1, gen_abst_emb_1)\n",
    "\n",
    "# Bigrams\n",
    "gen_abst_emb_2 = []\n",
    "\n",
    "for i in range(len(gen_abst_emb_1) - 1):\n",
    "    gen_abst_emb_2.append(gen_abst_emb_1[i] + \" \" + gen_abst_emb_1[i + 1])\n",
    "    \n",
    "print(\"\\nEvaluation of bigrams:\")\n",
    "evaluation_report(gol_abst_2, gen_abst_emb_2)\n",
    "\n",
    "# Trigrams\n",
    "gen_abst_emb_3 = []\n",
    "\n",
    "for i in range(len(gen_abst_emb_1) - 2):\n",
    "    gen_abst_emb_3.append(gen_abst_emb_1[i] + \" \" + gen_abst_emb_1[i + 1] +  \" \" + gen_abst_emb_1[i + 2])\n",
    "    \n",
    "print(\"\\nEvaluation of trigrams:\")\n",
    "evaluation_report(gol_abst_3, gen_abst_emb_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa7c2f",
   "metadata": {},
   "source": [
    "# Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29390be",
   "metadata": {},
   "source": [
    "### Reduce text using WordRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b5179e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sent in sentences:\n",
    "    [words.append(word) for word in sent.split()]\n",
    "    \n",
    "words = list(set(words))\n",
    "\n",
    "# Calculate the cosine similarity -- store result in a similarity matrix\n",
    "sim_mat = np.zeros((len(words), len(words)))\n",
    "for ind_1 in range(len(words)):\n",
    "    for ind_2 in range(ind_1, len(words)):\n",
    "        #sim_mat[ind_1, ind_2] = cosine_similarity(vectorizer.transform([words[ind_1]]), vectorizer.transform([words[ind_2]]))\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "sent_scores = {}\n",
    "\n",
    "for ind in range(len(sentences)):\n",
    "    w_score = 0\n",
    "    for word in sentences[ind].split():\n",
    "        w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "    sent_scores[ind] = w_score\n",
    "    \n",
    "sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e75b5",
   "metadata": {},
   "source": [
    "### Run TextRank on these 64% sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6963e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of unigrams:\n",
      "--------------ROUGE (Recall):\n",
      "36.36%\n",
      "--------------BLEU (Precision):\n",
      "33.78%\n",
      "--------------F1 score:\n",
      "35.03%\n",
      "\n",
      "Evaluation of bigrams:\n",
      "--------------ROUGE (Recall):\n",
      "11.22%\n",
      "--------------BLEU (Precision):\n",
      "9.59%\n",
      "--------------F1 score:\n",
      "10.34%\n",
      "\n",
      "Evaluation of trigrams:\n",
      "--------------ROUGE (Recall):\n",
      "2.06%\n",
      "--------------BLEU (Precision):\n",
      "1.39%\n",
      "--------------F1 score:\n",
      "1.66%\n"
     ]
    }
   ],
   "source": [
    "# Retrieve important sentences indices\n",
    "\n",
    "new_sent_ind = []\n",
    "count = 0\n",
    "\n",
    "for k in sorted_scores.keys():\n",
    "    if count > len(sorted_scores)*0.64:\n",
    "        break\n",
    "    else:\n",
    "        new_sent_ind.append(k)\n",
    "        count += 1\n",
    "\n",
    "# Calculate the cosine similarity -- store result in a similarity matrix\n",
    "sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "for ind_1 in range(len(new_sent_ind)):\n",
    "    for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(X[ind_1, :], X[ind_2, :])\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "            \n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph)\n",
    "\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "top_sent_hyb_pre = []\n",
    "top_sent_hyb_pro = []\n",
    "for k, v in sorted_scores.items():\n",
    "    if len(top_sent_hyb_pre) < len(abstract):\n",
    "\n",
    "        top_sent_hyb_pre.append(text[k])\n",
    "        top_sent_hyb_pro.append(sentences[k])\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "gen_abst_hyb = \" \".join(top_sent_hyb_pro)\n",
    "\n",
    "# Unigrams\n",
    "\n",
    "gen_abst_hyb_1 = gen_abst_hyb.split()\n",
    "\n",
    "print(\"\\nEvaluation of unigrams:\")\n",
    "evaluation_report(gol_abst_1, gen_abst_hyb_1)\n",
    "\n",
    "# Bigrams\n",
    "gen_abst_hyb_2 = []\n",
    "\n",
    "for i in range(len(gen_abst_hyb_1) - 1):\n",
    "    gen_abst_hyb_2.append(gen_abst_hyb_1[i] + \" \" + gen_abst_hyb_1[i + 1])\n",
    "    \n",
    "print(\"\\nEvaluation of bigrams:\")\n",
    "evaluation_report(gol_abst_2, gen_abst_hyb_2)\n",
    "\n",
    "# Trigrams\n",
    "gen_abst_hyb_3 = []\n",
    "\n",
    "\n",
    "for i in range(len(gen_abst_hyb_1) - 2):\n",
    "    gen_abst_hyb_3.append(gen_abst_hyb_1[i] + \" \" + gen_abst_hyb_1[i + 1] +  \" \" + gen_abst_hyb_1[i + 2])\n",
    "    \n",
    "print(\"\\nEvaluation of trigrams:\")\n",
    "evaluation_report(gol_abst_3, gen_abst_hyb_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dd355",
   "metadata": {},
   "source": [
    "### Run TextRank on these 80% sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8750cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of unigrams:\n",
      "--------------ROUGE (Recall):\n",
      "51.52%\n",
      "--------------BLEU (Precision):\n",
      "43.68%\n",
      "--------------F1 score:\n",
      "47.27%\n",
      "\n",
      "Evaluation of bigrams:\n",
      "--------------ROUGE (Recall):\n",
      "21.43%\n",
      "--------------BLEU (Precision):\n",
      "18.6%\n",
      "--------------F1 score:\n",
      "19.92%\n",
      "\n",
      "Evaluation of trigrams:\n",
      "--------------ROUGE (Recall):\n",
      "10.31%\n",
      "--------------BLEU (Precision):\n",
      "10.59%\n",
      "--------------F1 score:\n",
      "10.45%\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sent in sentences:\n",
    "    [words.append(word) for word in sent.split()]\n",
    "    \n",
    "words = list(set(words))\n",
    "\n",
    "# Calculate the cosine similarity -- store result in a similarity matrix\n",
    "sim_mat = np.zeros((len(words), len(words)))\n",
    "for ind_1 in range(len(words)):\n",
    "    for ind_2 in range(ind_1, len(words)):\n",
    "        #sim_mat[ind_1, ind_2] = cosine_similarity(vectorizer.transform([words[ind_1]]), vectorizer.transform([words[ind_2]]))\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(nlp.vocab[words[ind_1]].vector.reshape(1, 300), nlp.vocab[words[ind_2]].vector.reshape(1, 300))\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "        \n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph, tol=1.0e-2)\n",
    "\n",
    "sent_scores = {}\n",
    "\n",
    "for ind in range(len(sentences)):\n",
    "    w_score = 0\n",
    "    for word in sentences[ind].split():\n",
    "        w_score += scores[np.where(np.array(words) == word)[0][0]]\n",
    "    sent_scores[ind] = w_score\n",
    "    \n",
    "sorted_scores = dict(sorted(sent_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Retrieve important sentences indices\n",
    "\n",
    "new_sent_ind = []\n",
    "count = 0\n",
    "\n",
    "for k in sorted_scores.keys():\n",
    "    if count > len(sorted_scores)*0.8:\n",
    "        break\n",
    "    else:\n",
    "        new_sent_ind.append(k)\n",
    "        count += 1\n",
    "        \n",
    "# Calculate the cosine similarity -- store result in a similarity matrix\n",
    "sim_mat = np.zeros((len(new_sent_ind), len(new_sent_ind)))\n",
    "for ind_1 in range(len(new_sent_ind)):\n",
    "    for ind_2 in range(ind_1, len(new_sent_ind)):\n",
    "        sim_mat[ind_1, ind_2] = cosine_similarity(X[ind_1, :], X[ind_2, :])\n",
    "        sim_mat[ind_2, ind_1] = sim_mat[ind_1, ind_2]\n",
    "            \n",
    "sim_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(sim_graph)\n",
    "\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "top_sent_hyb_pre = []\n",
    "top_sent_hyb_pro = []\n",
    "for k, v in sorted_scores.items():\n",
    "    if len(top_sent_hyb_pre) < len(abstract):\n",
    "\n",
    "        top_sent_hyb_pre.append(text[k])\n",
    "        top_sent_hyb_pro.append(sentences[k])\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "gen_abst_hyb = \" \".join(top_sent_hyb_pro)\n",
    "\n",
    "# Unigrams\n",
    "\n",
    "gen_abst_hyb_1 = gen_abst_hyb.split()\n",
    "\n",
    "print(\"\\nEvaluation of unigrams:\")\n",
    "evaluation_report(gol_abst_1, gen_abst_hyb_1)\n",
    "\n",
    "# Bigrams\n",
    "gen_abst_hyb_2 = []\n",
    "\n",
    "for i in range(len(gen_abst_hyb_1) - 1):\n",
    "    gen_abst_hyb_2.append(gen_abst_hyb_1[i] + \" \" + gen_abst_hyb_1[i + 1])\n",
    "    \n",
    "print(\"\\nEvaluation of bigrams:\")\n",
    "evaluation_report(gol_abst_2, gen_abst_hyb_2)\n",
    "\n",
    "# Trigrams\n",
    "gen_abst_hyb_3 = []\n",
    "\n",
    "\n",
    "for i in range(len(gen_abst_hyb_1) - 2):\n",
    "    gen_abst_hyb_3.append(gen_abst_hyb_1[i] + \" \" + gen_abst_hyb_1[i + 1] +  \" \" + gen_abst_hyb_1[i + 2])\n",
    "    \n",
    "print(\"\\nEvaluation of trigrams:\")\n",
    "evaluation_report(gol_abst_3, gen_abst_hyb_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a602e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
